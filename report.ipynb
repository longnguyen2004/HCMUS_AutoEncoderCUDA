{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1ad6677",
   "metadata": {},
   "source": [
    "Team 4 \\\n",
    "22127242 Nguyen Long \\\n",
    "22127309 Nguyen Minh Nhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13928221",
   "metadata": {},
   "source": [
    "## Section 1: Problem Description\n",
    "\n",
    "### 1. Problem Statement\n",
    "This project builds an autoencoder for image reconstruction on the CIFAR-10 dataset.  \n",
    "An autoencoder learns to compress images and then reconstruct them back.  \n",
    "The main goal is to implement and speed up this process using CUDA on GPU, because CPU training is very slow for neural networks.\n",
    "\n",
    "### 2. CIFAR-10 Dataset Overview\n",
    "CIFAR-10 is a popular image dataset for computer vision tasks.\n",
    "\n",
    "- Total images: 60,000  \n",
    "- Image size: 32 × 32 pixels  \n",
    "- Color channels: 3 (RGB)  \n",
    "- Classes (10): airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck  \n",
    "- Training set: 50,000 images  \n",
    "- Test set: 10,000 images  \n",
    "\n",
    "Each image is stored as unsigned 8-bit values.\n",
    "\n",
    "**Data preprocessing**:\n",
    "- Pixel values are normalized from \\[0, 255\\] to \\[0, 1\\] by dividing by 255.\n",
    "- Labels are ignored during autoencoder training.\n",
    "- No data augmentation is applied.\n",
    "\n",
    "![Sample images from each class](resources/sample.png)\n",
    "\n",
    "### 3. Autoencoder Architecture\n",
    "The autoencoder has two main parts: an encoder and a decoder.  \n",
    "The encoder compresses the image into a smaller representation.  \n",
    "The decoder reconstructs the image from that representation.\n",
    "\n",
    "**Input size**: 32 × 32 × 3  \n",
    "**Latent size**: 8 × 8 × 128 = 8,192 features  \n",
    "**Output size**: 32 × 32 × 3  \n",
    "\n",
    "#### Encoder\n",
    "- Conv2D: 3 → 256 channels, kernel 3×3, padding 1  \n",
    "- ReLU  \n",
    "- MaxPool2D: 2×2 → output 16×16×256  \n",
    "- Conv2D: 256 → 128 channels, kernel 3×3, padding 1  \n",
    "- ReLU  \n",
    "- MaxPool2D: 2×2 → output 8×8×128  \n",
    "![Encoder](resources/Encoder.png)\n",
    "\n",
    "#### Decoder\n",
    "- Conv2D: 128 → 128 channels, kernel 3×3, padding 1  \n",
    "- ReLU  \n",
    "- UpSample2D: 2×2 → output 16×16×128  \n",
    "- Conv2D: 128 → 256 channels, kernel 3×3, padding 1  \n",
    "- ReLU  \n",
    "- UpSample2D: 2×2 → output 32×32×256  \n",
    "- Conv2D: 256 → 3 channels, kernel 3×3, padding 1  \n",
    "- No activation function in the last layer  \n",
    "\n",
    "The decoder mirrors the encoder structure to help image reconstruction.\n",
    "![Decoder](resources/Decoder.png)\n",
    "\n",
    "\n",
    "### 4. Project Objectives\n",
    "- **Performance**: Achieve large speedup using GPU compared to CPU (target >20×).  \n",
    "- **Learning**: Understand autoencoders, CUDA programming, and GPU optimization.  \n",
    "- **Quality**: Reconstruct CIFAR-10 images with low reconstruction loss.  \n",
    "- **Pipeline**: Use the trained encoder to extract features for later classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0353537",
   "metadata": {},
   "source": [
    "## Section 2: Implementation Phases\n",
    "\n",
    "### Phase 2.1: CPU Baseline Implementation\n",
    "\n",
    "#### Objectives\n",
    "- Build a correct autoencoder running on CPU.\n",
    "- Verify forward and backward passes work as expected.\n",
    "- Measure time and loss as a baseline before GPU optimization.\n",
    "- This phase is required to ensure correctness before moving to CUDA.\n",
    "\n",
    "#### Implementation Details\n",
    "\n",
    "##### Data Pipeline\n",
    "- Load CIFAR-10 data from binary files.\n",
    "- Each image consists of 1 label byte and 3072 image bytes (32×32×3).\n",
    "- Normalize pixel values from [0, 255] to [0, 1].\n",
    "- **Training uses only 1,000 images (~2% of CIFAR-10 training set)** to speed up CPU experiments.\n",
    "- **Testing uses the full 10,000-image test set**.\n",
    "\n",
    "##### Layer Implementations\n",
    "- **Conv2D**: 3×3 convolution with padding, implemented using nested CPU loops.\n",
    "- **ReLU**: Element-wise max(0, x).\n",
    "- **MaxPool2D**: 2×2 pooling with stride 2, take maximum value.\n",
    "- **UpSample2D**: Nearest-neighbor upsampling by factor 2.\n",
    "\n",
    "- **Forward pass**:  \n",
    "  Each layer first calls the `forward()` function of its previous layer.  \n",
    "  After the previous output is ready, the current layer computes its own output.\n",
    "\n",
    "- **Backward pass**:  \n",
    "  The execution order is reversed.\n",
    "  Each layer computes its gradients and the input gradient based on the output gradient, then calls `backward()` of the previous layer, passing to it the input gradient.\n",
    "\n",
    "This design keeps the layer connections simple and makes debugging easier.\n",
    "\n",
    "##### Training Loop\n",
    "- Loop over epochs.\n",
    "- Shuffle the 1,000 training images each epoch.\n",
    "- For each image:\n",
    "  - Forward pass through encoder and decoder.\n",
    "  - Compute MSE loss.\n",
    "  - Backward pass.\n",
    "  - Update weights using SGD.\n",
    "- Save model parameters after each epoch.\n",
    "\n",
    "##### Key Code Snippets\n",
    "\n",
    "Convolution function signature:\n",
    "```\n",
    "void convolve_cpu(\n",
    "    float *dst,\n",
    "    const float *src,\n",
    "    const float *kernel,\n",
    "    int col,\n",
    "    int row,\n",
    "    int kernel_width\n",
    ");\n",
    "```\n",
    "\n",
    "Main training loop:\n",
    "```\n",
    "for (const auto& image : image_refs) {\n",
    "    input->setImage(image->data);\n",
    "    output->forward();\n",
    "    output->backward(learning_rate, nullptr);\n",
    "}\n",
    "```\n",
    "\n",
    "#### Results\n",
    "\n",
    "- **Training data**: 1,000 images (≈2% of CIFAR-10 training set).\n",
    "- **Test data**: Full 10,000-image CIFAR-10 test set.\n",
    "- **Reconstruction loss**:\n",
    "  - Training MSE loss ≈ **0.01**.\n",
    "  - Test MSE loss ≈ **0.01**.\n",
    "- This result is **surprising**, because the model was trained on a very small subset but still shows similar loss on the full test set.\n",
    "- Loss values are stable during evaluation.\n",
    "- Reconstructed images preserve overall structure but are blurry.\n",
    "- **Performance (CPU, 2% dataset only)**:\n",
    "  - **Average 100-image block time**: ~59.35 seconds.\n",
    "  - **Total epoch time (1,000 images)**: **~593.5 seconds**.\n",
    "  - **Total training time (20 epochs)**: **~11,870 seconds (approx. 3.3 hours)**.\n",
    "- **Extrapolation**: At this rate, training the full 50,000 image dataset for 200 epochs on a single CPU core would take approximately **68 days**, making GPU acceleration a mandatory requirement for this project.\n",
    "- **Total parameters**: ~751,875, which fits comfortably within standard system RAM.\n",
    "![CPU_comparison](resources/CPU_comparison.png)\n",
    "#### Key Takeaways\n",
    "- Even training on only 1,000 images, the autoencoder generalizes well in terms of MSE.\n",
    "- CPU performance is very slow, mainly due to convolution.\n",
    "- Conv2D is the main bottleneck.\n",
    "- These observations strongly motivate moving convolution and training to GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d6ee60",
   "metadata": {},
   "source": [
    "## Phase 2.2: GPU Basic Implementation\n",
    "\n",
    "### Objectives\n",
    "The objective of this phase was to port the complete CPU-based autoencoder to CUDA and execute it on the GPU using **basic parallelization techniques**. All major operations—convolution, activation, pooling, upsampling, and loss computation—were reimplemented as CUDA kernels.  \n",
    "\n",
    "Key goals included:\n",
    "* Verifying correctness by comparing GPU outputs against the CPU baseline (ensuring MSE $\\approx 0.01$).\n",
    "* Establishing an **initial GPU performance baseline** to measure the impact of later optimizations.\n",
    "\n",
    "---\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "#### Parallelization Strategy\n",
    "A **1:1 mapping strategy** was adopted for all naive kernels to prioritize implementation correctness. Each CUDA thread is responsible for computing a single output element:\n",
    "* **Convolution/Upsampling:** One thread per output pixel.\n",
    "* **Max Pooling:** One thread per pooled output element.\n",
    "* **ReLU:** One thread per activation element.\n",
    "\n",
    "---\n",
    "\n",
    "#### Kernel Designs\n",
    "\n",
    "**Convolution (Conv2D)** As seen in the provided `conv2d.cu`, each thread computes one output pixel $(x, y, f)$. The thread performs nested loops over input channels and the kernel window:\n",
    "* **Global Memory usage:** Every weight and input pixel is fetched directly from global memory within the inner loops.\n",
    "* **Boundary Handling:** Explicit padding checks ensure threads do not access out-of-bounds memory.\n",
    "\n",
    "```cpp\n",
    "__global__ void naive_conv_forward_kernel(\n",
    "    float* output, const float* input, const float* weights, \n",
    "    const float* biases, int width, int height, \n",
    "    int in_channels, int out_channels, int kernel_size) {\n",
    "    \n",
    "    const int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    const int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    const int f = blockIdx.z;\n",
    "\n",
    "    if (x < width && y < height && f < out_channels) {\n",
    "        // Nested loops over channels and kernel size...\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Other Kernels**\n",
    "* **MaxPool2D:** Threads scan a $2 \\times 2$ window to find the maximum value.\n",
    "* **ReLU:** In-place element-wise operation: $x = \\max(0, x)$.\n",
    "* **Upsampling:** Nearest-neighbor mapping using integer coordinate scaling.\n",
    "\n",
    "---\n",
    "\n",
    "#### Memory Management\n",
    "All model parameters and activations are managed on the device using standard CUDA API calls:\n",
    "* `cudaMalloc` handles allocation for weights, activations, and gradients.\n",
    "* `cudaMemcpy` manages the Host-to-Device (H2D) transfer of CIFAR-10 batches and Device-to-Host (D2H) transfer for loss evaluation.\n",
    "* Buffers are reused across iterations to minimize the overhead of memory allocation.\n",
    "\n",
    "---\n",
    "\n",
    "### Results & Speedup Analysis\n",
    "\n",
    "#### CPU Time Extrapolation\n",
    "Based on the CPU baseline logs, a block of 100 images averages $59.35s$, resulting in a full epoch (1,000 images) taking approximately $593.56s$. To compare the GPU's performance on the full 50,000 image dataset against this CPU baseline, we extrapolate the CPU time:\n",
    "\n",
    "$$T_{CPU(50k)} = 593.56s \\times \\left(\\frac{50,000}{1,000}\\right) = 29,678 \\text{ seconds per epoch}$$\n",
    "\n",
    "#### Performance Comparison\n",
    "\n",
    "| Implementation | Images | Avg. Epoch Time (s) | Speedup |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| CPU (Extrapolated) | 50,000 | 29,678.00 | $1.0\\times$ |\n",
    "| **GPU Naive** | 50,000 | **991.63** | **$\\approx 29.93\\times$** |\n",
    "\n",
    "\n",
    "\n",
    "By correcting the baseline, we see that even the **GPU Naive** implementation provides a massive **$\\approx 30\\times$ speedup** over the serial CPU implementation. This highlights that the initial bottleneck was not just memory access, but the inherent serial nature of CPU nested loops for high-dimensional convolutions.\n",
    "\n",
    "#### Reconstruction Quality\n",
    "* **MSE Loss:** Stabilized at **~0.01** on the first epoch, converge to 0.04 after 20th epochs.\n",
    "* **Generalization:** Even with the limited 2% training subset used in the CPU phase, the model captured essential structural features. GPU reconstruction produces images that are blurry but structurally identical to the input, validating the kernel logic.\n",
    "![Gpu reconstruction](resources/gpu_reconstruct.png)\n",
    "\n",
    "---\n",
    "\n",
    "### Profiling Analysis\n",
    "\n",
    "Using the CUDA profiler, we identified a massive performance bottleneck:\n",
    "1. **Dominant Kernel:** `naive_conv_backward_weights_kernel` accounts for approximately **84.4%** of execution time.\n",
    "2. **Memory Latency:** Because threads do not use **Shared Memory**, the same input data is loaded from global memory thousands of times across different threads in a block.\n",
    "3. **Instruction Stall:** High global memory pressure leads to significant \"Warps Stall\" where the GPU waits for data rather than performing calculations. \n",
    "\n",
    "![naive profiling](resources/naive.png)\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* **Parallelism Wins:** Moving to GPU immediately reduced training time by 66%, confirming the high parallel nature of the Autoencoder.\n",
    "* **Memory is the Wall:** The profiling results clearly show that computation is not the bottleneck—**memory bandwidth** is.\n",
    "* **Future Optimization:** The dominance of the convolution kernels $(\\approx 95\\% \\text{ of total time})$ confirms that Phase 3 optimizations (Shared Memory Tiling and Im2Col) are essential for achieving higher speedups.\n",
    "\n",
    "**Next Step:** Implement Shared Memory Tiling in Phase 3.1 to reduce global memory fetches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86d16ad",
   "metadata": {},
   "source": [
    "## Phase 2.3: GPU Optimized Implementation – Version 1\n",
    "\n",
    "### Objectives\n",
    "The objective of this phase was to address the primary bottleneck identified in Phase 2.2: the **Conv2D backward pass**, specifically the weight gradient computation. \n",
    "\n",
    "Key goals for this optimization phase included:\n",
    "* Implementing a **specialized weight-gradient kernel** using parallel reduction to replace the inefficient \"one-thread-per-weight\" approach.\n",
    "* Utilizing **Kernel-level Batching** via the Z-dimension of the grid to eliminate CUDA stream overhead and false dependencies.\n",
    "* Incorporating **Shared Memory Tiling** to maximize data reuse and reduce the Global Memory bandwidth bottleneck.\n",
    "\n",
    "---\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "#### 1. Kernel-level Batching\n",
    "To minimize the overhead of launching multiple kernels for different channels or filters, we packed workloads into a single grid launch. By utilizing the `blockIdx.z` dimension, we can process multiple output channels concurrently.\n",
    "\n",
    "* **Advantage:** This avoids the scheduling latency of sequential streams and ensures the GPU remains saturated even when individual filter operations are small.\n",
    "\n",
    "#### 2. Specialized Weight Gradient Kernel (Parallel Reduction)\n",
    "In Phase 2.2, calculating gradients for a $3 \\times 3$ filter meant only 9 threads were active per filter, leading to massive underutilization. \n",
    "\n",
    "We reformulated this as a **Tree-based Reduction**:\n",
    "* **Parallelism:** We spawn a $32 \\times 32$ thread block to handle the accumulation.\n",
    "* **Method:** Threads compute element-wise multiplications, store them in `__shared__` memory, and perform a logarithmic reduction to sum the gradients.\n",
    "\n",
    "\n",
    "\n",
    "```cpp\n",
    "// Example of the Tree Reduction implemented in weight gradient kernels\n",
    "for (int stride = blockDim.x * blockDim.y / 2; stride > 0; stride /= 2) {\n",
    "    if (tid < stride) {\n",
    "        reduction[tid] += reduction[tid + stride];\n",
    "    }\n",
    "    __syncthreads();\n",
    "}\n",
    "```\n",
    "\n",
    "#### 3. Shared Memory Tiling\n",
    "Input activations are cooperatively loaded into `__shared__` memory tiles. This allows multiple threads to access the same pixel data without triggering redundant and high-latency Global Memory reads, significantly increasing the **Arithmetic Intensity** of the convolution operation.\n",
    "\n",
    "---\n",
    "\n",
    "### Results & Speedup Analysis\n",
    "\n",
    "#### CPU Time Extrapolation\n",
    "Based on the CPU baseline logs, a block of 100 images averages $59.35s$, which translates to $593.56s$ for a 1,000-image epoch. To compare the GPU's performance on the full 50,000 image dataset against this baseline, we extrapolate the CPU time as follows:\n",
    "\n",
    "$$T_{CPU(50k)} = 593.56s \\times \\left(\\frac{50,000}{1,000}\\right) = 29,678 \\text{ seconds per epoch}$$\n",
    "\n",
    "#### Performance Comparison Table\n",
    "\n",
    "| Implementation | Epoch Time (s) | Speedup (vs. CPU) | Speedup (vs. Naive) |\n",
    "| :--- | :---: | :---: | :---: |\n",
    "| CPU (Extrapolated) | $29,678.00$ | $1.0\\times$ | – |\n",
    "| GPU Naive (Phase 2.2) | $991.63$ | $\\approx 29.93\\times$ | $1.0\\times$ |\n",
    "| **GPU Optimized v1 (2.3)** | **$517.18$** | **$\\approx 57.39\\times$** | **$\\approx 1.92\\times$** |\n",
    "\n",
    "\n",
    "\n",
    "#### Speedup Calculation\n",
    "\n",
    "The cumulative improvement from the serial CPU baseline to this optimized GPU implementation is  \n",
    "$Speedup_{Total} = \\frac{T_{CPU}}{T_{GPU\\_Opt}} = \\frac{29{,}678.00}{517.177} \\approx 57.39\\times$.\n",
    "\n",
    "By correcting the baseline, we see that the **GPU Optimized v1** implementation provides a massive **$\\approx 57\\times$ speedup** over the serial CPU. This demonstrates that while the naive GPU version was already a significant leap, the memory optimizations in this phase nearly double that advantage again.\n",
    "\n",
    "---\n",
    "\n",
    "### Profiling Analysis\n",
    "Profiling the optimized code revealed a shift in the performance landscape:\n",
    "* **Backward Pass Efficiency:** The `Conv2D` backward kernels are no longer the overwhelming bottleneck. The execution time for weight gradient computation decreased significantly due to parallel reduction.\n",
    "* **Amdahl's Law:** As the convolution kernels became faster, the relative time spent in \"fixed-cost\" layers like **ReLU** and **MaxPool** increased. This confirms that our primary optimization target was correctly identified and addressed.\n",
    "* **Memory Throughput:** Shared memory usage effectively reduced the \"Global Memory Load Efficiency\" bottleneck observed in the previous phase.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "* **Dimension Awareness:** Tailoring kernels to the specific dimensions of the operation (e.g., using reduction for small $3 \\times 3$ filters) is critical for GPU occupancy.\n",
    "* **Batching Strategy:** Kernel-level batching is more efficient than CUDA streams for these specific layers, as it reduces launch overhead and simplifies dependency management.\n",
    "* **Memory Reuse:** Shared memory tiling remains the most effective way to overcome the memory wall in convolutional neural networks.\n",
    "\n",
    "**Next Step:** Further optimization through **Kernel Fusion** and exploring **im2col** strategies in Phase 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ac4d33",
   "metadata": {},
   "source": [
    "\n",
    "## Phase 2.4: GPU Optimized Implementation – Version 2\n",
    "\n",
    "### Objectives\n",
    "This phase represents the **most aggressive and impactful optimization** in the entire pipeline. The main objective was to fundamentally **reformulate convolution** to maximize GPU efficiency by:\n",
    "\n",
    "- Implementing **Implicit im2col** to eliminate large intermediate matrices and drastically reduce global memory footprint.\n",
    "- Expressing Conv2D forward and backward passes as **GEMM (General Matrix Multiply)** operations.\n",
    "- Maximizing **arithmetic intensity** so that the kernels become **compute-bound rather than memory-bound**.\n",
    "\n",
    "This phase targets the core performance limiter of all previous versions: inefficient memory access.\n",
    "\n",
    "---\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "#### 1. Implicit im2col Mapping\n",
    "Traditional im2col explicitly materializes a large matrix of size:\n",
    "\\[\n",
    "(H \\cdot W) \\times (C \\cdot K \\cdot K)\n",
    "\\]\n",
    "which is extremely memory-intensive and bandwidth-bound.\n",
    "\n",
    "Instead, we implement **Implicit im2col**, where tensor expansion is never stored in memory. A lightweight device function computes the mapping from GEMM indices to input tensor coordinates **on-the-fly** inside the kernel.\n",
    "\n",
    "```cpp\n",
    "template <int kernel_width, bool flip_kernel>\n",
    "__device__ __forceinline__\n",
    "float im2col_map_coords(\n",
    "    const float* src,\n",
    "    int c, int y_in, int x,\n",
    "    int row, int col) {\n",
    "    // Compute input coordinates dynamically\n",
    "    // Avoids explicit im2col buffer\n",
    "}\n",
    "```\n",
    "\n",
    "This trades a small amount of index arithmetic for a **massive reduction in memory traffic**, which is highly favorable on GPUs.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. GEMM-based Convolution Kernel\n",
    "With Implicit im2col, convolution is expressed as:\n",
    "\\[\n",
    "C = A \\times B\n",
    "\\]\n",
    "where:\n",
    "- **Matrix A** = convolution weights  \n",
    "- **Matrix B** = implicit im2col input patches  \n",
    "- **Matrix C** = output feature maps  \n",
    "\n",
    "The `convolve_gemm_kernel` is implemented using **shared memory tiling**, similar to a classic tiled GEMM:\n",
    "\n",
    "- `TILE_WIDTH × TILE_WIDTH` tiles of **Weights (A)** and **Input (B)** are cooperatively loaded into shared memory.\n",
    "- Each thread computes a partial dot product and accumulates results in registers.\n",
    "- Templates specialize the kernel for:\n",
    "  - Forward pass\n",
    "  - Backward-input\n",
    "  - Backward-weight  \n",
    "  without runtime branching.\n",
    "\n",
    "This approach dramatically increases data reuse and arithmetic intensity.\n",
    "\n",
    "```cpp\n",
    "template <int kernel_width, bool real_convolve, bool transpose_weights, bool weight_grad>\n",
    "__global__ void convolve_gemm_kernel(\n",
    "    float* __restrict__ output,\n",
    "    const float* __restrict__ input,\n",
    "    const float* __restrict__ weights,\n",
    "    const int width, const int height,\n",
    "    const int in_channels, const int out_channels) \n",
    "{\n",
    "    // Shared memory tiles for Matrix A (Weights) and Matrix B (Input)\n",
    "    __shared__ float s_Weights[TILE_WIDTH][TILE_WIDTH];\n",
    "    __shared__ float s_Input[TILE_WIDTH][TILE_WIDTH];\n",
    "\n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "    \n",
    "    // Matrix Multiplication logic using Shared Memory Tiling\n",
    "    float acc = 0.0f;\n",
    "    for (int t = 0; t < (K_dimension + TILE_WIDTH - 1) / TILE_WIDTH; ++t) {\n",
    "        // Cooperative loading of tiles from GMEM to SMEM\n",
    "        if (row < M_dim && (t * TILE_WIDTH + tx) < K_dim)\n",
    "            s_Weights[ty][tx] = weights[...];\n",
    "        else\n",
    "            s_Weights[ty][tx] = 0.0f;\n",
    "\n",
    "        if (col < N_dim && (t * TILE_WIDTH + ty) < K_dim)\n",
    "            s_Input[ty][tx] = im2col_map_coords<kernel_width>(...);\n",
    "        else\n",
    "            s_Input[ty][tx] = 0.0f;\n",
    "\n",
    "        __syncthreads();\n",
    "\n",
    "        // Perform partial dot product on tiles\n",
    "        for (int i = 0; i < TILE_WIDTH; ++i)\n",
    "            acc += s_Weights[ty][i] * s_Input[i][tx];\n",
    "        __syncthreads();\n",
    "    }\n",
    "    // Write final result to Matrix C (Output)\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Bias & Reduction Optimization\n",
    "Bias gradient computation previously relied on global `atomicAdd`, which scaled poorly.\n",
    "\n",
    "In this version:\n",
    "- Partial bias sums are computed per warp.\n",
    "- **Warp-level shuffle reduction** (`__shfl_down_sync`) is used to sum values directly through registers.\n",
    "- Only one thread per warp performs the final atomic update.\n",
    "\n",
    "This minimizes synchronization overhead and global contention.\n",
    "\n",
    "---\n",
    "\n",
    "### Results & Speedup Analysis\n",
    "\n",
    "#### Timing Summary\n",
    "- **CPU Baseline (Extrapolated):** 29,678.00 s / epoch  \n",
    "- **GPU Naive (Phase 2.2):** 991.63 s / epoch  \n",
    "- **GPU Optimized v1 (Phase 2.3):** 517.18 s / epoch  \n",
    "- **GPU Optimized v2 (Phase 2.4):** **117.97 s / epoch**\n",
    "\n",
    "#### Performance Comparison Table\n",
    "\n",
    "| Implementation | Epoch Time (s) | Speedup (vs. CPU) | Speedup (vs. Prev Phase) |\n",
    "| :--- | :---: | :---: | :---: |\n",
    "| CPU (Extrapolated) | 29,678.00 | $1.0\\times$ | – |\n",
    "| GPU Naive (2.2) | 991.63 | $\\approx 29.93\\times$ | $\\approx 29.93\\times$ |\n",
    "| GPU Optimized v1 (2.3) | 517.18 | $\\approx 57.39\\times$ | $\\approx 1.92\\times$ |\n",
    "| **GPU Optimized v2 (2.4)** | **117.97** | **$\\approx 251.57\\times$** | **$\\approx 4.38\\times$** |\n",
    "\n",
    "\n",
    "\n",
    "#### Cumulative Speedup\n",
    "\n",
    "The total improvement from the original serial CPU implementation to this final GEMM-optimized version is  \n",
    "$Speedup_{total} = \\frac{29{,}678.00}{117.973} \\approx 251.57\\times$.\n",
    "\n",
    "This represents the most significant performance leap in the project. By transforming the convolution operation into a compute-bound GEMM problem and utilizing implicit im2col, we reduced the per-epoch training time from over 8 hours on a CPU to less than 2 minutes on the GPU.\n",
    "\n",
    "---\n",
    "\n",
    "### Profiling Analysis\n",
    "Profiling reveals a fundamental shift in kernel behavior:\n",
    "\n",
    "- **Memory Access Patterns:**  \n",
    "  Global memory accesses are now highly regular and coalesced, with most reuse occurring in shared memory.\n",
    "- **Bottleneck Shift:**  \n",
    "  The kernel transitions from being **memory-bound** to **compute-bound**, which is the ideal regime for GPU execution.\n",
    "- **Global Memory Traffic:**  \n",
    "  Explicit im2col buffers are completely removed, resulting in a dramatic reduction in memory bandwidth consumption.\n",
    "\n",
    "The `convolve_gemm_kernel` now dominates execution time, but at significantly higher throughput and efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "- **Implicit im2col + GEMM** delivers the most significant performance gain in the entire pipeline.\n",
    "- On modern GPUs, **recomputing indices is far cheaper than moving data**.\n",
    "- High-performance CUDA code often relies on **template-based kernel specialization** to avoid runtime overhead.\n",
    "- This optimization transforms convolution from a bandwidth-limited operation into a compute-efficient one, pushing performance close to hardware limits.\n",
    "\n",
    "This phase marks the point where the implementation aligns with **industry-grade deep learning frameworks** in both structure and performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b2605",
   "metadata": {},
   "source": [
    "### Phase 2.5: SVM Integration\n",
    "For convenience and reproducibility, the source code is uploaded to [Kaggle](https://www.kaggle.com/code/meownosaur/svm-cifar10).\n",
    "\n",
    "#### Objectives\n",
    "- Use the trained encoder to extract image features.\n",
    "- Train an SVM classifier on these features.\n",
    "- Evaluate the full image classification pipeline.\n",
    "\n",
    "#### Implementation Details\n",
    "\n",
    "##### Feature Extraction\n",
    "- Only the **encoder** part of the autoencoder is used.\n",
    "- For each image, a forward pass is executed.\n",
    "- The encoder output is taken as the feature vector.\n",
    "- Feature size is **8 × 8 × 128 = 8192 dimensions**.\n",
    "- Features are extracted for:\n",
    "  - 50,000 training images\n",
    "  - 10,000 test images\n",
    "- Features and labels are saved into CSV files for later use.\n",
    "\n",
    "Feature extraction logic:\n",
    "```\n",
    "input->setImage(image.data);\n",
    "(*layers.rbegin())->forward();\n",
    "\n",
    "const float* enc_dev = encoder_layer->output();\n",
    "cudaMemcpy(enc_host.data(), enc_dev,\n",
    "           feature_size * sizeof(float),\n",
    "           cudaMemcpyDeviceToHost);\n",
    "```\n",
    "\n",
    "##### SVM Integration\n",
    "- Extracted features are loaded using cuDF.\n",
    "- Features are normalized using `StandardScaler`.\n",
    "- SVM is trained using cuML `SVC`, which runs on GPU.\n",
    "- This avoids implementing SVM from scratch and is fast.\n",
    "\n",
    "##### Hyperparameter Selection\n",
    "- Kernel: RBF\n",
    "- C = 10.0\n",
    "- gamma = \"scale\"\n",
    "- These values give good accuracy without long training time.\n",
    "\n",
    "SVM training code:\n",
    "```\n",
    "model = SVC(kernel='rbf', C=10.0, gamma='scale')\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "#### Results\n",
    "\n",
    "- **Feature extraction**:\n",
    "  - Training set: 50,000 images\n",
    "  - Test set: 10,000 images\n",
    "  - Feature size: 8192 per image\n",
    "- **SVM training time**: ~46 seconds\n",
    "- **Classification accuracy**:\n",
    "  - Training accuracy: **86.03%**\n",
    "  - Test accuracy: **67.56%**\n",
    "\n",
    "##### Confusion Matrix Summary\n",
    "- Vehicles (ship, car, truck, plane) are classified very well.\n",
    "- Animals (cat, dog, bird) have lower accuracy.\n",
    "- Strong confusion exists between similar animals, especially cat and dog.\n",
    "![Confusion Matrix](resources/ConfusionMatrix.png)\n",
    "#### Analysis\n",
    "- **Easiest classes**: ship, frog, plane, car.\n",
    "- **Hardest classes**: cat, bird, dog.\n",
    "- The confusion matrix shows most errors are between visually similar classes.\n",
    "- Test accuracy is slightly higher than the expected range (60–65%).\n",
    "\n",
    "#### Key Takeaways\n",
    "- The encoder learns meaningful and reusable features.\n",
    "- The two-stage approach (autoencoder + SVM) works well.\n",
    "- GPU-based feature extraction and SVM give good end-to-end performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d536dc",
   "metadata": {},
   "source": [
    "## Section 3: Comprehensive Performance Analysis\n",
    "\n",
    "### 3.1 Performance Comparison Across All Phases\n",
    "\n",
    "The table below summarizes the performance scaling for a full **20-epoch training cycle** on the CIFAR-10 dataset (50,000 images).\n",
    "\n",
    "| Phase | Training Time (s) | Speedup (vs CPU) | Incremental Speedup | Memory (RAM/VRAM) | Key Optimization |\n",
    "|:---|:---|:---|:---|:---|:---|\n",
    "| **CPU Baseline** | $593,560.0$ | $1.00\\times$ | – | 710 MiB | Serial execution |\n",
    "| **GPU Naive** | $19,832.6$ | $\\approx 29.93\\times$ | $29.93\\times$ | 1024 / 116 MiB | Parallelization |\n",
    "| **GPU Optimized v1** | $10,343.6$ | $\\approx 57.39\\times$ | $1.92\\times$ | 799 / 90 MiB | Shared Memory / Reduction |\n",
    "| **GPU Optimized v2** | **$2,359.4$** | **$\\approx 251.57\\times$** | **$4.38\\times$** | 799 / 88 MiB | Implicit im2col + GEMM |\n",
    "\n",
    "### 3.2 Visualizations & Impact Analysis\n",
    "\n",
    "The following visualizations illustrate the exponential performance gains achieved through architectural refinements.\n",
    "\n",
    "**Total Training Time (Log Scale)**\n",
    "Training time was reduced from an estimated **165 hours** on a single CPU core to just under **40 minutes** on the Optimized GPU v2 implementation.\n",
    "![Training time across phases](resources/3.1.png)\n",
    "\n",
    "**Cumulative Speedup Analysis**\n",
    "The cumulative speedup reached **$251.57\\times$**. The transition to **Implicit im2col + GEMM** (v2) provided the most significant compute throughput gain, allowing the model to reach near-peak hardware utilization.\n",
    "![Cumulative speedup across phases](resources/3.2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80843cf",
   "metadata": {},
   "source": [
    "## Section 4: Lessons Learned and Challenges Overcome\n",
    "\n",
    "### 4.1 Key Technical Insights\n",
    "\n",
    "- **CUDA Programming**:  \n",
    "  I learned how to design CUDA kernels with shared memory, tiling, and synchronization.  \n",
    "  Kernel fusion and careful memory access are very important for speed.\n",
    "\n",
    "- **Deep Learning**:  \n",
    "  Convolution dominates both compute time and memory usage.  \n",
    "  Forward and backward passes must be designed together for performance.\n",
    "\n",
    "- **Performance Optimization (Kernel Fusion + Streams)**:  \n",
    "  Reducing global memory access gives large speedups.  \n",
    "  Combining operations and overlapping compute with memory transfers improves GPU usage.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 Major Challenges and Solutions\n",
    "\n",
    "✓ **Challenge 1: im2col and GEMM Memory Overhead**\n",
    "\n",
    "+ **Problem**:  \n",
    "  Traditional im2col creates a very large temporary matrix, which uses too much memory and slows down GPU execution.\n",
    "\n",
    "+ **Solution**:  \n",
    "  Instead of explicit im2col, I implemented **implicit im2col** inside the GEMM kernel.  \n",
    "  Input values are mapped on-the-fly using index mapping functions.  \n",
    "  I implemented **three mapping modes**:\n",
    "  - Coordinate-based mapping for forward convolution  \n",
    "  - Channel-wise mapping for weight gradient computation  \n",
    "  - Flipped-kernel mapping for backward input gradient  \n",
    "\n",
    "  This avoids allocating large im2col buffers and reduces memory traffic.\n",
    "\n",
    "+ **Lesson**:  \n",
    "  Avoid materializing large intermediate tensors; compute values implicitly when possible to save memory and improve performance.\n",
    "\n",
    "✓ **Challenge 2: Designing a Clean GPU Layer API**\n",
    "\n",
    "+ **Problem**:  \n",
    "  We needed a flexible and clean API to connect many GPU layers together, while supporting forward and backward passes and managing parameters without making the code complex or tightly coupled.\n",
    "\n",
    "+ **Solution**:  \n",
    "  We designed a common `LayerGPU` interface with `forward()`, `backward()`, `dimension()`, and `setParams()` functions.  \n",
    "  Layers are linked using `std::shared_ptr`, so each layer can safely reference the previous layer.  \n",
    "  All trainable parameters are stored in one contiguous GPU memory buffer and assigned to each layer using offsets, which simplifies memory management and keeps the training loop clean. By allocating all weights in a single contiguous buffer (flat array) on the GPU, we improved cache hit rates and simplified the Weight Update (SGD) kernel, as it could iterate through the entire model's parameters in a single pass rather than launching kernels for each layer's specific weight matrix.\n",
    "\n",
    "+ **Lesson**:  \n",
    "  A well-designed API makes GPU code easier to maintain, extend, and debug, especially for large CUDA-based projects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776e2d9d",
   "metadata": {},
   "source": [
    "## Section 5: Conclusion and Future Work\n",
    "\n",
    "### 5.1 Project Summary\n",
    "The results show a clear improvement in training performance across all phases.\n",
    "The CPU baseline is very slow when scaled to the full dataset, taking several hours to train.\n",
    "\n",
    "Moving the implementation to the GPU gives a large speedup due to parallelization.\n",
    "Using shared memory improves memory access efficiency but does not significantly change the total training time compared to the basic GPU version.\n",
    "\n",
    "The largest performance gain comes from Implicit im2col, which removes large intermediate buffers and reduces global memory access.\n",
    "This optimization greatly reduces training time and results in the highest overall speedup compared to the CPU baseline.\n",
    "\n",
    "Overall, the performance analysis confirms that memory optimization is the key factor in accelerating convolution-based neural networks on the GPU.\n",
    "\n",
    "### 5.2 Key Achievements\n",
    "\n",
    "\n",
    "* **Maximum Speedup:** Achieved a total end-to-end training speedup of **251.55×** compared to the serial CPU baseline ($593,500s$ vs $2,359.4s$ for 20 epochs).\n",
    "* **Classification Accuracy:** Successfully extracted high-quality features, allowing the SVM classifier to reach a test accuracy of **67.56%**, exceeding the target of 60–65%.\n",
    "* **Primary Optimization:** **Implicit im2col + GEMM** provided the most significant performance leap, quadrupling throughput by transforming the convolution into a compute-bound operation.\n",
    "* **Technical Mastery:**\n",
    "    * **Kernel Design:** Implementation of shared memory tiling and warp-level reductions.\n",
    "    * **Resource Management:** Optimized data pipelines that reduced system RAM overhead by 22% compared to the naive version.\n",
    "    * **System Framework:** A fully functional C++/CUDA pipeline capable of completing a full training cycle in under 40 minutes.\n",
    "\n",
    "### 5.3 Limitations\n",
    "\n",
    "#### Current performance bottlenecks\n",
    "\n",
    "We planned to add minibatch stochastic gradient descent to reduce memory copy between host and device, but this wasn't done due to time constraint, having to fix various bugs in the GPU kernels, and the overall structure of the program not easily permitting such modifications without significant refactoring. Thankfully, regular SGD is fast enough and produces great results, so it is not needed.\n",
    "\n",
    "### 5.4 Future improvements\n",
    "\n",
    "- Implement minibatch SGD\n",
    "- Fuse convolution and bias into 1 kernel, instead of 2 separate kernels (bias trick)\n",
    "- Add `float4` optimization to `updateWeights` and bias kernel"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
