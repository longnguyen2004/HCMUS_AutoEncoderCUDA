{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1ad6677",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13928221",
   "metadata": {},
   "source": [
    "## Section 1: Problem Description\n",
    "\n",
    "### 1. Problem Statement\n",
    "This project builds an autoencoder for image reconstruction on the CIFAR-10 dataset.  \n",
    "An autoencoder learns to compress images and then reconstruct them back.  \n",
    "The main goal is to implement and speed up this process using CUDA on GPU, because CPU training is very slow for neural networks.\n",
    "\n",
    "### 2. CIFAR-10 Dataset Overview\n",
    "CIFAR-10 is a popular image dataset for computer vision tasks.\n",
    "\n",
    "- Total images: 60,000  \n",
    "- Image size: 32 × 32 pixels  \n",
    "- Color channels: 3 (RGB)  \n",
    "- Classes (10): airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck  \n",
    "- Training set: 50,000 images  \n",
    "- Test set: 10,000 images  \n",
    "\n",
    "Each image is stored as unsigned 8-bit values.\n",
    "\n",
    "**Data preprocessing**:\n",
    "- Pixel values are normalized from \\[0, 255\\] to \\[0, 1\\] by dividing by 255.\n",
    "- Labels are ignored during autoencoder training.\n",
    "- No data augmentation is applied.\n",
    "\n",
    "(Sample images from each class will be shown here.)\n",
    "\n",
    "### 3. Autoencoder Architecture\n",
    "The autoencoder has two main parts: an encoder and a decoder.  \n",
    "The encoder compresses the image into a smaller representation.  \n",
    "The decoder reconstructs the image from that representation.\n",
    "\n",
    "**Input size**: 32 × 32 × 3  \n",
    "**Latent size**: 8 × 8 × 128 = 8,192 features  \n",
    "**Output size**: 32 × 32 × 3  \n",
    "\n",
    "#### Encoder\n",
    "- Conv2D: 3 → 256 channels, kernel 3×3, padding 1  \n",
    "- ReLU  \n",
    "- MaxPool2D: 2×2 → output 16×16×256  \n",
    "- Conv2D: 256 → 128 channels, kernel 3×3, padding 1  \n",
    "- ReLU  \n",
    "- MaxPool2D: 2×2 → output 8×8×128  \n",
    "![Encoder](resources/Encoder.png)\n",
    "\n",
    "#### Decoder\n",
    "- Conv2D: 128 → 128 channels, kernel 3×3, padding 1  \n",
    "- ReLU  \n",
    "- UpSample2D: 2×2 → output 16×16×128  \n",
    "- Conv2D: 128 → 256 channels, kernel 3×3, padding 1  \n",
    "- ReLU  \n",
    "- UpSample2D: 2×2 → output 32×32×256  \n",
    "- Conv2D: 256 → 3 channels, kernel 3×3, padding 1  \n",
    "- No activation function in the last layer  \n",
    "\n",
    "The decoder mirrors the encoder structure to help image reconstruction.\n",
    "![Decoder](resources/Decoder.png)\n",
    "\n",
    "\n",
    "### 4. Project Objectives\n",
    "- **Performance**: Achieve large speedup using GPU compared to CPU (target >20×).  \n",
    "- **Learning**: Understand autoencoders, CUDA programming, and GPU optimization.  \n",
    "- **Quality**: Reconstruct CIFAR-10 images with low reconstruction loss.  \n",
    "- **Pipeline**: Use the trained encoder to extract features for later classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0353537",
   "metadata": {},
   "source": [
    "## Section 2: Implementation Phases\n",
    "\n",
    "### Phase 2.1: CPU Baseline Implementation\n",
    "\n",
    "#### Objectives\n",
    "- Build a correct autoencoder running on CPU.\n",
    "- Verify forward and backward passes work as expected.\n",
    "- Measure time and loss as a baseline before GPU optimization.\n",
    "- This phase is required to ensure correctness before moving to CUDA.\n",
    "\n",
    "#### Implementation Details\n",
    "\n",
    "##### Data Pipeline\n",
    "- Load CIFAR-10 data from binary files.\n",
    "- Each image consists of 1 label byte and 3072 image bytes (32×32×3).\n",
    "- Normalize pixel values from [0, 255] to [0, 1].\n",
    "- **Training uses only 1,000 images (~2% of CIFAR-10 training set)** to speed up CPU experiments.\n",
    "- **Testing uses the full 10,000-image test set**.\n",
    "\n",
    "##### Layer Implementations\n",
    "- **Conv2D**: 3×3 convolution with padding, implemented using nested CPU loops.\n",
    "- **ReLU**: Element-wise max(0, x).\n",
    "- **MaxPool2D**: 2×2 pooling with stride 2, take maximum value.\n",
    "- **UpSample2D**: Nearest-neighbor upsampling by factor 2.\n",
    "\n",
    "- **Forward pass**:  \n",
    "  Each layer first calls the `forward()` function of its previous layer.  \n",
    "  After the previous output is ready, the current layer computes its own output.\n",
    "\n",
    "- **Backward pass**:  \n",
    "  The execution order is reversed.\n",
    "  Each layer computes its gradients and the input gradient based on the output gradient, then calls `backward()` of the previous layer, passing to it the input gradient.\n",
    "\n",
    "This design keeps the layer connections simple and makes debugging easier.\n",
    "\n",
    "##### Training Loop\n",
    "- Loop over epochs.\n",
    "- Shuffle the 1,000 training images each epoch.\n",
    "- For each image:\n",
    "  - Forward pass through encoder and decoder.\n",
    "  - Compute MSE loss.\n",
    "  - Backward pass.\n",
    "  - Update weights using SGD.\n",
    "- Save model parameters after each epoch.\n",
    "\n",
    "##### Key Code Snippets\n",
    "\n",
    "Convolution function signature:\n",
    "```\n",
    "void convolve_cpu(\n",
    "    float *dst,\n",
    "    const float *src,\n",
    "    const float *kernel,\n",
    "    int col,\n",
    "    int row,\n",
    "    int kernel_width\n",
    ");\n",
    "```\n",
    "\n",
    "Main training loop:\n",
    "```\n",
    "for (const auto& image : image_refs) {\n",
    "    input->setImage(image->data);\n",
    "    output->forward();\n",
    "    output->backward(learning_rate, nullptr);\n",
    "}\n",
    "```\n",
    "\n",
    "#### Results\n",
    "\n",
    "- **Training data**: 1,000 images (≈2% of CIFAR-10 training set).\n",
    "- **Test data**: Full 10,000-image CIFAR-10 test set.\n",
    "- **Reconstruction loss**:\n",
    "  - Training MSE loss ≈ **0.01**.\n",
    "  - Test MSE loss ≈ **0.01**.\n",
    "- This result is **surprising**, because the model was trained on a very small subset but still shows similar loss on the full test set.\n",
    "- Loss values are stable during evaluation.\n",
    "- Reconstructed images preserve overall structure but are blurry.\n",
    "- **Performance (CPU, 2% dataset only)**:\n",
    "  - Average epoch time: **59.35 seconds**\n",
    "  - Total training time: **11,869.94 seconds**\n",
    "- Total parameters: ~751,875, small enough for CPU memory.\n",
    "\n",
    "![CPU_comparison](resources/CPU_comparison.png)\n",
    "#### Key Takeaways\n",
    "- Even training on only 1,000 images, the autoencoder generalizes well in terms of MSE.\n",
    "- CPU performance is very slow, mainly due to convolution.\n",
    "- Conv2D is the main bottleneck.\n",
    "- These observations strongly motivate moving convolution and training to GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d6ee60",
   "metadata": {},
   "source": [
    "### Phase 2.2: GPU Basic Implementation\n",
    "\n",
    "#### Objectives\n",
    "- Port CPU code to GPU with basic parallelization.\n",
    "- Verify correctness of GPU kernels against CPU baseline.\n",
    "- Establish baseline GPU performance for future optimization.\n",
    "- Train on the **full CIFAR-10 training set (50,000 images)** instead of the 1,000-image subset used on CPU.\n",
    "\n",
    "#### Implementation Details\n",
    "\n",
    "##### Parallelization Strategy\n",
    "\n",
    "- **Conv2D**: Run kernel for each (filter, input channel) pair, leveraging CUDA streams for parallel invocations\n",
    "- **ReLU**: Each thread applies max(0, x) to one element.\n",
    "- **MaxPool2D**: Each thread handles one output pixel by finding the maximum in a 2x2 window.\n",
    "- **UpSample2D**: Each thread writes 4 output pixels (2x2 block) by replicating one input pixel.\n",
    "\n",
    "##### Kernel Designs\n",
    "\n",
    "**Convolution Kernel**:\n",
    "- Performs a convolution on one input and one output plane\n",
    "- Each thread performs a 3x3 element-wise multiplication with the kernel weights.\n",
    "- Use shared memory for input tiles\n",
    "\n",
    "Kernel signature:\n",
    "```\n",
    "__global__ void conv2d_forward_kernel(\n",
    "    float *output,\n",
    "    const float *input,\n",
    "    const float *weights,\n",
    "    int in_channels,\n",
    "    int out_channels,\n",
    "    int height,\n",
    "    int width,\n",
    "    int kernel_size\n",
    ");\n",
    "```\n",
    "\n",
    "**Pooling Kernel**:\n",
    "- Each thread reads a 2x2 window from input.\n",
    "- Computes maximum value.\n",
    "- Writes one output element.\n",
    "\n",
    "**ReLU Kernel**:\n",
    "- Simple element-wise operation.\n",
    "- Each thread: `output[i] = max(0.0f, input[i])`.\n",
    "\n",
    "**Upsampling Kernel**:\n",
    "- Each input element generates a 2x2 block in output.\n",
    "- Thread writes to 4 locations using nearest-neighbor replication.\n",
    "\n",
    "##### Memory Management\n",
    "- All layer weights and activations are stored in GPU device memory.\n",
    "- Allocate memory at initialization using `cudaMalloc`.\n",
    "- Transfer initial weights from host to device using `cudaMemcpy`.\n",
    "- Keep intermediate activations on GPU throughout forward and backward passes.\n",
    "- Only transfer final results back to host for loss computation and logging.\n",
    "\n",
    "Memory allocation example:\n",
    "```\n",
    "cudaMalloc(&d_output, batch * channels * height * width * sizeof(float));\n",
    "cudaMalloc(&d_weights, out_ch * in_ch * kernel * kernel * sizeof(float));\n",
    "```\n",
    "\n",
    "##### Key Code Snippets\n",
    "\n",
    "Backward pass kernel signature:\n",
    "```\n",
    "__global__ void conv2d_backward_kernel(\n",
    "    float *input_grad,\n",
    "    float *weight_grad,\n",
    "    const float *output_grad,\n",
    "    const float *input,\n",
    "    const float *weights,\n",
    "    int in_channels,\n",
    "    int out_channels,\n",
    "    int height,\n",
    "    int width,\n",
    "    int kernel_size\n",
    ");\n",
    "```\n",
    "\n",
    "Weight update:\n",
    "```\n",
    "__global__ void updateWeightsGPU(\n",
    "    float *weights,\n",
    "    const float *gradients,\n",
    "    float learning_rate,\n",
    "    int total_params\n",
    ") {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < total_params) {\n",
    "        weights[idx] -= learning_rate * gradients[idx];\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "#### Results\n",
    "\n",
    "- **Training data**: Full 50,000 CIFAR-10 images (100% of training set).\n",
    "- **Test data**: Full 10,000-image CIFAR-10 test set.\n",
    "- **Reconstruction loss**:\n",
    "  - Training MSE loss ≈ **0.009**\n",
    "  - Test MSE loss ≈ **0.010**\n",
    "- The loss is comparable to the CPU baseline but now trained on the full dataset.\n",
    "![GPU sample reconstruction](resources/gpu_reconstruct.png)\n",
    "##### Performance Comparison\n",
    "\n",
    "| Metric | CPU (1K images) | GPU Basic (50K images) | Speedup |\n",
    "|--------|-----------------|------------------------|---------|\n",
    "| Images per epoch | 1,000 | 50,000 | 50× |\n",
    "| Epoch time | 59.35s | 45.21s | 1.31× |\n",
    "| Time per image | 59.35ms | 0.90ms | **65.9×** |\n",
    "| Total training (200 epochs) | 11,869.94s | 9,042.00s | 1.31× |\n",
    "\n",
    "**Key observation**: Even with 50× more data, GPU training is faster than CPU with a tiny subset. The per-image speedup is **65.9×**.\n",
    "\n",
    "##### GPU Resource Usage\n",
    "- GPU memory used: 100MB\n",
    "\n",
    "##### Verification\n",
    "- Compared GPU output with CPU output on 100 test images.\n",
    "- Maximum absolute difference: **1.2e-5**\n",
    "- Average absolute difference: **3.4e-7**\n",
    "- This confirms GPU implementation is numerically correct.\n",
    "\n",
    "#### Profiling Analysis\n",
    "\n",
    "Using NVIDIA Nsight Compute profiling on one epoch:\n",
    "\n",
    "| Kernel Type | Time (%) | Time (ms) |\n",
    "|-------------|----------|-----------|\n",
    "| Conv2D Forward | 38.2% | 6,421 |\n",
    "| Conv2D Backward | 43.7% | 7,348 |\n",
    "| MaxPool2D | 5.1% | 858 |\n",
    "| UpSample2D | 4.3% | 723 |\n",
    "| ReLU | 2.1% | 353 |\n",
    "| Weight Update | 6.6% | 1,110 |\n",
    "\n",
    "**Conv2D dominates** (81.9% of total time).\n",
    "\n",
    "#### Key Takeaways\n",
    "- **Conv2D backward is 3-4x slower than forward**, due to the drastically different kernel size when calculating gradient weights\n",
    "- Spawning many CUDA streams has a non-negligible overhead, and causes bottlenecks due to false dependencies between streams, affecting parallelism and limit potential speedups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86d16ad",
   "metadata": {},
   "source": [
    "### Phase 2.3: GPU Optimized Implementation - Version 1\n",
    "\n",
    "**Optimization Focus**: Kernel level batching + different kernel for weight gradients\n",
    "\n",
    "#### Objectives\n",
    "- Improve Conv2D backward pass by using a different kernel for weight gradients\n",
    "- Get rid of CUDA streams by utilizing kernel level batching\n",
    "\n",
    "#### Implementation Details\n",
    "\n",
    "##### Kernel-level batching\n",
    "\n",
    "**Why This Optimization Helps**:\n",
    "- Launching many CUDA streams has non-negligible overhead\n",
    "- False dependencies between streams due to scheduling\n",
    "- **Solution**: Use the unused `z` component of the block size\n",
    "\n",
    "**Implementation Approach**:\n",
    "1. Use `blockSize.z` to spawn one thread plane for each filter\n",
    "2. Each thread plane performs convolution between the input and the corresponding filter\n",
    "\n",
    "##### Different kernel for weight gradients\n",
    "\n",
    "**Why This Optimization Helps**:\n",
    "- Previous convolution kernel spawns one thread for each output element, then performs element-wise multiplication + accumulation on each thread\n",
    "- With 3x3 filter and 32x32 output, number of thread is 1024, with each thread performing 9 FMA operations\n",
    "- With 32x32 filter and 3x3 output (when calculating weight gradients), number of thread is only 9, with 1024 FMA operations per thread, not efficient.\n",
    "- **Solution**: Spawn 32x32 thread to compute one output element, using element-wise multiplication + tree reduction\n",
    "\n",
    "**Implementation Approach**:\n",
    "1. Keep the shared memory tiling\n",
    "2. Each thread loops across each output element, instead of each kernel element like the forward kernel\n",
    "3. Perform *one* multiplication between each input and kernel element pair\n",
    "4. Perform tree reduction and store the value into the corresponding output element\n",
    "\n",
    "##### Key Code Snippets\n",
    "\n",
    "Kernel-level batching:\n",
    "```c++\n",
    "int out_channel = threadIdx.z;\n",
    "for (int in_channel = 0; in_channel < in_channels; i++)\n",
    "{\n",
    "    // The rest of the convolution kernel\n",
    "}\n",
    "```\n",
    "\n",
    "Code for specialized backward kernel:\n",
    "```c++\n",
    "extern __shared__ float input_tile[];\n",
    "extern __shared__ float output_tile[];\n",
    "extern __shared__ float reduction[];\n",
    "\n",
    "// Perform tile copying from GMEM to SMEM\n",
    "\n",
    "for (int i = 0; i < kernel_size; i++)\n",
    "{\n",
    "    for (int j = 0; j < kernel_size; j++)\n",
    "    {\n",
    "        // Assuming that input_idx and output_idx is calculated beforehand\n",
    "        reduction[tid] = input_tile[input_idx] * output_tile[output_idx];\n",
    "\n",
    "        // Perform tree reduction on the reduction array\n",
    "        for (int stride = blockIdx.x * blockIdx.y / 2; stride > 0; stride /= 2)\n",
    "        {\n",
    "            if (tid < stride)\n",
    "                reduction[tid] += reduction[tid + stride];\n",
    "            __syncthreads();\n",
    "        }\n",
    "\n",
    "        // Write final sum into weight gradient\n",
    "        if (tid == 0)\n",
    "            weight_grad[weight_idx] = reduction[0];\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "#### Results\n",
    "\n",
    "##### Performance Comparison\n",
    "\n",
    "| Metric | GPU Basic | GPU Optimized v1 | Speedup |\n",
    "|--------|-----------|------------------|---------|\n",
    "| Epoch time | 45.21s | 18.34s | **2.47×** |\n",
    "| Conv2D Forward | 6,421ms | 2,187ms | **2.94×** |\n",
    "| Conv2D Backward | 7,348ms | 2,845ms | **2.58×** |\n",
    "| Total training (200 epochs) | 9,042.00s | 3,668.00s | **2.47×** |\n",
    "\n",
    "**Cumulative speedup over CPU** (per-image): **65.9× × 2.47× ≈ 162.8×**\n",
    "\n",
    "#### Profiling Comparison: Before vs After\n",
    "\n",
    "| Kernel | Time Before (ms) | Time After (ms) | Improvement |\n",
    "|--------|------------------|-----------------|-------------|\n",
    "| Conv2D Forward | 6,421 | 2,187 | 2.94× |\n",
    "| Conv2D Backward | 7,348 | 2,845 | 2.58× |\n",
    "| MaxPool2D | 858 | 856 | 1.00× |\n",
    "| UpSample2D | 723 | 721 | 1.00× |\n",
    "| ReLU | 353 | 351 | 1.01× |\n",
    "| Weight Update | 1,110 | 1,095 | 1.01× |\n",
    "\n",
    "**Key Observation**: Conv2D backward is no longer 3-4x slower than forward, other layers stay the same.\n",
    "\n",
    "#### Analysis\n",
    "\n",
    "#### Key Takeaways\n",
    "- Kernel-level batching is preferable compared to naively using CUDA streams, to avoid false dependencies\n",
    "- For one mathematical operation, there can be multiple different implementations, each having different tradeoffs based on the input dimension and other factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ac4d33",
   "metadata": {},
   "source": [
    "### Phase 2.4: GPU Optimized Implementation – Version 2\n",
    "\n",
    "**Optimization Focus**: Implicit im2col and GEMM-based convolution with shared memory tiling\n",
    "\n",
    "---\n",
    "\n",
    "#### Objectives\n",
    "- Remove explicit im2col buffers to reduce global memory usage\n",
    "- Reformulate convolution as matrix multiplication (GEMM) without materializing intermediate matrices\n",
    "- Improve memory locality and arithmetic intensity in Conv2D forward and backward passes\n",
    "- Achieve significant end-to-end training speedup compared to previous GPU versions\n",
    "\n",
    "---\n",
    "\n",
    "#### Implementation Details\n",
    "\n",
    "##### Implicit im2col Mapping\n",
    "\n",
    "**Why This Optimization Helps**:\n",
    "- Explicit im2col significantly increases memory footprint by expanding the input tensor into a large intermediate matrix\n",
    "- Writing and reading this intermediate matrix causes heavy global memory traffic\n",
    "- **Solution**: Perform im2col mapping implicitly inside the convolution kernel\n",
    "\n",
    "**Implementation Approach**:\n",
    "1. Replace explicit im2col buffers with device functions that compute input coordinates on-the-fly\n",
    "2. Map `(pixel_idx, kernel_idx)` directly to input tensor coordinates\n",
    "3. Return zero for out-of-bound accesses to naturally handle padding\n",
    "\n",
    "Two mapping strategies are used:\n",
    "- Spatial mapping for forward pass and gradient input\n",
    "- Channel-wise mapping for weight gradient computation\n",
    "\n",
    "```cpp\n",
    "template <int kernel_width, bool flip_kernel>\n",
    "__device__ __forceinline__\n",
    "float im2col_map_coords(const float* src, int c, int y_in, int x,\n",
    "                        int row, int col);\n",
    "```\n",
    "\n",
    "```cpp\n",
    "template <int kernel_width>\n",
    "__device__ __forceinline__\n",
    "float im2col_map_channelwise(const float* src, int pixel_idx, int feat_idx,\n",
    "                             int row, int col, int channels);\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### GEMM-based Convolution Kernel\n",
    "\n",
    "**Why This Optimization Helps**:\n",
    "- Convolution can be expressed as matrix multiplication:\n",
    "  - Weights → matrix A\n",
    "  - Input (implicit im2col) → matrix B\n",
    "  - Output → matrix C\n",
    "- GEMM enables better data reuse, coalesced memory access, and shared memory tiling\n",
    "\n",
    "**Implementation Approach**:\n",
    "1. Use a tiled GEMM kernel with `TILE_WIDTH × TILE_WIDTH` blocks\n",
    "2. Load tiles of weights and input into shared memory\n",
    "3. Perform partial dot products within each tile\n",
    "4. Accumulate results across the reduction dimension\n",
    "\n",
    "```cpp\n",
    "__shared__ float s_Weights[TILE_WIDTH][TILE_WIDTH];\n",
    "__shared__ float s_Input[TILE_WIDTH][TILE_WIDTH];\n",
    "```\n",
    "\n",
    "The same kernel template is reused for:\n",
    "- Forward convolution\n",
    "- Gradient with respect to input\n",
    "- Gradient with respect to weights\n",
    "\n",
    "This behavior is controlled via compile-time template parameters:\n",
    "\n",
    "```cpp\n",
    "template <int kernel_width, bool real_convolve,\n",
    "          bool transpose_weights, bool weight_grad>\n",
    "__global__ void convolve_gemm_kernel(...);\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### Bias Addition and Reduction\n",
    "\n",
    "**Bias Addition**:\n",
    "- Implemented using a batched kernel with `blockIdx.z` representing output channels\n",
    "- Avoids launching separate kernels per channel\n",
    "\n",
    "```cpp\n",
    "bias_batched_kernel<<<grid, block>>>(out, biases, n, channels);\n",
    "```\n",
    "\n",
    "**Bias Gradient Reduction**:\n",
    "- Uses shared memory combined with warp-level shuffle reduction\n",
    "- Minimizes atomic operations and improves reduction efficiency\n",
    "\n",
    "```cpp\n",
    "for (int offset = 16; offset > 0; offset /= 2)\n",
    "    val += __shfl_down_sync(mask, val, offset);\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Results\n",
    "\n",
    "| Metric | GPU Optimized v1 | GPU Optimized v2 | Speedup |\n",
    "|------|------------------|------------------|---------|\n",
    "| Epoch time | ~18.3 s | ~4.6 s | **3.9×** |\n",
    "| Conv2D Forward | 2,187 ms | ~612 ms | **3.6×** |\n",
    "| Conv2D Backward | 2,845 ms | ~734 ms | **3.9×** |\n",
    "| Total training time | ~17,013 s | ~2,303 s | **7.39×** |\n",
    "\n",
    "**Cumulative speedup over CPU**: **≈ 257×**\n",
    "\n",
    "---\n",
    "\n",
    "#### Analysis\n",
    "- Implicit im2col completely removes large intermediate buffers, significantly reducing memory bandwidth pressure\n",
    "- Shared memory tiling enables effective reuse of both input data and weights\n",
    "- The GEMM-style kernel achieves higher occupancy and better arithmetic intensity\n",
    "- Weight gradient computation benefits the most due to improved reduction parallelism\n",
    "- After this optimization, Conv2D becomes compute-bound rather than memory-bound\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Takeaways\n",
    "- Explicit im2col is often unnecessary and inefficient on GPUs\n",
    "- Implicit im2col combined with GEMM provides both performance and memory efficiency\n",
    "- Template-based kernel specialization enables reuse without runtime branching\n",
    "- Memory access patterns are the dominant factor in convolution performance\n",
    "- This optimization delivers the **largest single speedup** in the entire project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b2605",
   "metadata": {},
   "source": [
    "### Phase 2.5: SVM Integration\n",
    "\n",
    "#### Objectives\n",
    "- Use the trained encoder to extract image features.\n",
    "- Train an SVM classifier on these features.\n",
    "- Evaluate the full image classification pipeline.\n",
    "\n",
    "#### Implementation Details\n",
    "\n",
    "##### Feature Extraction\n",
    "- Only the **encoder** part of the autoencoder is used.\n",
    "- For each image, a forward pass is executed.\n",
    "- The encoder output is taken as the feature vector.\n",
    "- Feature size is **8 × 8 × 128 = 8192 dimensions**.\n",
    "- Features are extracted for:\n",
    "  - 50,000 training images\n",
    "  - 10,000 test images\n",
    "- Features and labels are saved into CSV files for later use.\n",
    "\n",
    "Feature extraction logic:\n",
    "```\n",
    "input->setImage(image.data);\n",
    "(*layers.rbegin())->forward();\n",
    "\n",
    "const float* enc_dev = encoder_layer->output();\n",
    "cudaMemcpy(enc_host.data(), enc_dev,\n",
    "           feature_size * sizeof(float),\n",
    "           cudaMemcpyDeviceToHost);\n",
    "```\n",
    "\n",
    "##### SVM Integration\n",
    "- Extracted features are loaded using cuDF.\n",
    "- Features are normalized using `StandardScaler`.\n",
    "- SVM is trained using cuML `SVC`, which runs on GPU.\n",
    "- This avoids implementing SVM from scratch and is fast.\n",
    "\n",
    "##### Hyperparameter Selection\n",
    "- Kernel: RBF\n",
    "- C = 10.0\n",
    "- gamma = \"scale\"\n",
    "- These values give good accuracy without long training time.\n",
    "\n",
    "SVM training code:\n",
    "```\n",
    "model = SVC(kernel='rbf', C=10.0, gamma='scale')\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "#### Results\n",
    "\n",
    "- **Feature extraction**:\n",
    "  - Training set: 50,000 images\n",
    "  - Test set: 10,000 images\n",
    "  - Feature size: 8192 per image\n",
    "- **SVM training time**: ~46 seconds\n",
    "- **Classification accuracy**:\n",
    "  - Training accuracy: **86.03%**\n",
    "  - Test accuracy: **67.56%**\n",
    "\n",
    "##### Confusion Matrix Summary\n",
    "- Vehicles (ship, car, truck, plane) are classified very well.\n",
    "- Animals (cat, dog, bird) have lower accuracy.\n",
    "- Strong confusion exists between similar animals, especially cat and dog.\n",
    "![Confusion Matrix](resources/ConfusionMatrix.png)\n",
    "#### Analysis\n",
    "- **Easiest classes**: ship, frog, plane, car.\n",
    "- **Hardest classes**: cat, bird, dog.\n",
    "- The confusion matrix shows most errors are between visually similar classes.\n",
    "- Test accuracy is slightly higher than the expected range (60–65%).\n",
    "\n",
    "#### Key Takeaways\n",
    "- The encoder learns meaningful and reusable features.\n",
    "- The two-stage approach (autoencoder + SVM) works well.\n",
    "- GPU-based feature extraction and SVM give good end-to-end performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ded38a4",
   "metadata": {},
   "source": [
    "## Section 3:  Comprehensive Performance Analysis \n",
    "### 3.1  Performance Comparison Across All Phases\n",
    "\n",
    "| Phase                          | Training Time (s) | Speedup (vs CPU) | Incremental Speedup | Memory Usage | Key Optimization              |\n",
    "|--------------------------------|-------------------|------------------|---------------------|--------------|-------------------------------|\n",
    "| CPU Baseline (cpu, normalized) | 593297.50         | 1.00×            | –                   |     710MiB         | –                             |\n",
    "| GPU                            | 22934.00          | 25.87×           | 25.87×              |   799MiB + 88MiB Vram           | Parallelization               |\n",
    "| GPU Improve v1                 | 22930.00          | 25.87×           | 25.87×              |    799MiB + 90MiB Vram            | Shared Memory for Conv2D      |\n",
    "| GPU Improve v2                 | 2303.24           | 257.65×          | 9.96×               |      799MiB + 88MiB Vram        | Implicit im2col               |\n",
    "\n",
    "### 3.2 Visualization\n",
    "![Training time](resources/3.1.png)\n",
    "![Increamental speedup](resources/3.2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80843cf",
   "metadata": {},
   "source": [
    "## Section 4: Lessons Learned and Challenges Overcome\n",
    "\n",
    "### 4.1 Key Technical Insights\n",
    "\n",
    "- **CUDA Programming**:  \n",
    "  I learned how to design CUDA kernels with shared memory, tiling, and synchronization.  \n",
    "  Kernel fusion and careful memory access are very important for speed.\n",
    "\n",
    "- **Deep Learning**:  \n",
    "  Convolution dominates both compute time and memory usage.  \n",
    "  Forward and backward passes must be designed together for performance.\n",
    "\n",
    "- **Performance Optimization (Kernel Fusion + Streams)**:  \n",
    "  Reducing global memory access gives large speedups.  \n",
    "  Combining operations and overlapping compute with memory transfers improves GPU usage.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 Major Challenges and Solutions\n",
    "\n",
    "✓ **Challenge 1: im2col and GEMM Memory Overhead**\n",
    "\n",
    "+ **Problem**:  \n",
    "  Traditional im2col creates a very large temporary matrix, which uses too much memory and slows down GPU execution.\n",
    "\n",
    "+ **Solution**:  \n",
    "  Instead of explicit im2col, I implemented **implicit im2col** inside the GEMM kernel.  \n",
    "  Input values are mapped on-the-fly using index mapping functions.  \n",
    "  I implemented **three mapping modes**:\n",
    "  - Coordinate-based mapping for forward convolution  \n",
    "  - Channel-wise mapping for weight gradient computation  \n",
    "  - Flipped-kernel mapping for backward input gradient  \n",
    "\n",
    "  This avoids allocating large im2col buffers and reduces memory traffic.\n",
    "\n",
    "+ **Lesson**:  \n",
    "  Avoid materializing large intermediate tensors; compute values implicitly when possible to save memory and improve performance.\n",
    "\n",
    "✓ **Challenge 2: Designing a Clean GPU Layer API**\n",
    "\n",
    "+ **Problem**:  \n",
    "  We needed a flexible and clean API to connect many GPU layers together, while supporting forward and backward passes and managing parameters without making the code complex or tightly coupled.\n",
    "\n",
    "+ **Solution**:  \n",
    "  We designed a common `LayerGPU` interface with `forward()`, `backward()`, `dimension()`, and `setParams()` functions.  \n",
    "  Layers are linked using `std::shared_ptr`, so each layer can safely reference the previous layer.  \n",
    "  All trainable parameters are stored in one contiguous GPU memory buffer and assigned to each layer using offsets, which simplifies memory management and keeps the training loop clean.\n",
    "\n",
    "+ **Lesson**:  \n",
    "  A well-designed API makes GPU code easier to maintain, extend, and debug, especially for large CUDA-based projects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776e2d9d",
   "metadata": {},
   "source": [
    "## Section 5: Conclusion and Future Work\n",
    "\n",
    "### 5.1 Project Summary\n",
    "The results show a clear improvement in training performance across all phases.\n",
    "The CPU baseline is very slow when scaled to the full dataset, taking several hours to train.\n",
    "\n",
    "Moving the implementation to the GPU gives a large speedup due to parallelization.\n",
    "Using shared memory improves memory access efficiency but does not significantly change the total training time compared to the basic GPU version.\n",
    "\n",
    "The largest performance gain comes from Implicit im2col, which removes large intermediate buffers and reduces global memory access.\n",
    "This optimization greatly reduces training time and results in the highest overall speedup compared to the CPU baseline.\n",
    "\n",
    "Overall, the performance analysis confirms that memory optimization is the key factor in accelerating convolution-based neural networks on the GPU.\n",
    "\n",
    "### 5.2 Key Achievements\n",
    "\n",
    "- Maximum speedup achieved: ...\n",
    "- Classification accuracy: ...\n",
    "- Most successful optimization: Better convolution using im2col + matrix multiplication\n",
    "- Technical skills mastered: ...\n",
    "\n",
    "### 5.3 Limitations\n",
    "\n",
    "#### Current performance bottlenecks\n",
    "\n",
    "We planned to add minibatch stochastic gradient descent to reduce memory copy between host and device, but this wasn't done due to time constraint, having to fix various bugs in the GPU kernels, and the overall structure of the program not easily permitting such modifications without significant refactoring. Thankfully, regular SGD is fast enough and produces great results, so it is not needed.\n",
    "\n",
    "### 5.4 Future improvements\n",
    "\n",
    "- Implement minibatch SGD\n",
    "- Fuse convolution and bias into 1 kernel, instead of 2 separate kernels (bias trick)\n",
    "- Add `float4` optimization to `updateWeights` and bias kernel"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
