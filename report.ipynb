{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1ad6677",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13928221",
   "metadata": {},
   "source": [
    "## Section 1: Problem Description\n",
    "\n",
    "### 1. Problem Statement\n",
    "This project builds an autoencoder for image reconstruction on the CIFAR-10 dataset.  \n",
    "An autoencoder learns to compress images and then reconstruct them back.  \n",
    "The main goal is to implement and speed up this process using CUDA on GPU, because CPU training is very slow for neural networks.\n",
    "\n",
    "### 2. CIFAR-10 Dataset Overview\n",
    "CIFAR-10 is a popular image dataset for computer vision tasks.\n",
    "\n",
    "- Total images: 60,000  \n",
    "- Image size: 32 × 32 pixels  \n",
    "- Color channels: 3 (RGB)  \n",
    "- Classes (10): airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck  \n",
    "- Training set: 50,000 images  \n",
    "- Test set: 10,000 images  \n",
    "\n",
    "Each image is stored as unsigned 8-bit values.\n",
    "\n",
    "**Data preprocessing**:\n",
    "- Pixel values are normalized from \\[0, 255\\] to \\[0, 1\\] by dividing by 255.\n",
    "- Labels are ignored during autoencoder training.\n",
    "- No data augmentation is applied.\n",
    "\n",
    "(Sample images from each class will be shown here.)\n",
    "\n",
    "### 3. Autoencoder Architecture\n",
    "The autoencoder has two main parts: an encoder and a decoder.  \n",
    "The encoder compresses the image into a smaller representation.  \n",
    "The decoder reconstructs the image from that representation.\n",
    "\n",
    "**Input size**: 32 × 32 × 3  \n",
    "**Latent size**: 8 × 8 × 128 = 8,192 features  \n",
    "**Output size**: 32 × 32 × 3  \n",
    "\n",
    "#### Encoder\n",
    "- Conv2D: 3 → 256 channels, kernel 3×3, padding 1  \n",
    "- ReLU  \n",
    "- MaxPool2D: 2×2 → output 16×16×256  \n",
    "- Conv2D: 256 → 128 channels, kernel 3×3, padding 1  \n",
    "- ReLU  \n",
    "- MaxPool2D: 2×2 → output 8×8×128  \n",
    "![Encoder](resources/Encoder.png)\n",
    "\n",
    "#### Decoder\n",
    "- Conv2D: 128 → 128 channels, kernel 3×3, padding 1  \n",
    "- ReLU  \n",
    "- UpSample2D: 2×2 → output 16×16×128  \n",
    "- Conv2D: 128 → 256 channels, kernel 3×3, padding 1  \n",
    "- ReLU  \n",
    "- UpSample2D: 2×2 → output 32×32×256  \n",
    "- Conv2D: 256 → 3 channels, kernel 3×3, padding 1  \n",
    "- No activation function in the last layer  \n",
    "\n",
    "The decoder mirrors the encoder structure to help image reconstruction.\n",
    "![Decoder](resources/Decoder.png)\n",
    "\n",
    "\n",
    "### 4. Project Objectives\n",
    "- **Performance**: Achieve large speedup using GPU compared to CPU (target >20×).  \n",
    "- **Learning**: Understand autoencoders, CUDA programming, and GPU optimization.  \n",
    "- **Quality**: Reconstruct CIFAR-10 images with low reconstruction loss.  \n",
    "- **Pipeline**: Use the trained encoder to extract features for later classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0353537",
   "metadata": {},
   "source": [
    "## Section 2: Implementation Phases\n",
    "\n",
    "### Phase 2.1: CPU Baseline Implementation\n",
    "\n",
    "#### Objectives\n",
    "- Build a correct autoencoder running on CPU.\n",
    "- Verify forward and backward passes work as expected.\n",
    "- Measure time and loss as a baseline before GPU optimization.\n",
    "- This phase is required to ensure correctness before moving to CUDA.\n",
    "\n",
    "#### Implementation Details\n",
    "\n",
    "##### Data Pipeline\n",
    "- Load CIFAR-10 data from binary files.\n",
    "- Each image consists of 1 label byte and 3072 image bytes (32×32×3).\n",
    "- Normalize pixel values from [0, 255] to [0, 1].\n",
    "- **Training uses only 1,000 images (~2% of CIFAR-10 training set)** to speed up CPU experiments.\n",
    "- **Testing uses the full 10,000-image test set**.\n",
    "\n",
    "##### Layer Implementations\n",
    "- **Conv2D**: 3×3 convolution with padding, implemented using nested CPU loops.\n",
    "- **ReLU**: Element-wise max(0, x).\n",
    "- **MaxPool2D**: 2×2 pooling with stride 2, take maximum value.\n",
    "- **UpSample2D**: Nearest-neighbor upsampling by factor 2.\n",
    "\n",
    "- **Forward pass**:  \n",
    "  Each layer first calls the `forward()` function of its previous layer.  \n",
    "  After the previous output is ready, the current layer computes its own output.\n",
    "\n",
    "- **Backward pass**:  \n",
    "  The execution order is reversed.  \n",
    "  Each layer calls the `backward()` function of the next layer first, then computes its own gradients.\n",
    "\n",
    "This design keeps the layer connections simple and makes debugging easier.\n",
    "\n",
    "##### Training Loop\n",
    "- Loop over epochs.\n",
    "- Shuffle the 1,000 training images each epoch.\n",
    "- For each image:\n",
    "  - Forward pass through encoder and decoder.\n",
    "  - Compute MSE loss.\n",
    "  - Backward pass.\n",
    "  - Update weights using SGD.\n",
    "- Save model parameters after each epoch.\n",
    "\n",
    "##### Key Code Snippets\n",
    "\n",
    "Convolution function signature:\n",
    "```\n",
    "void convolve_cpu(\n",
    "    float *dst,\n",
    "    const float *src,\n",
    "    const float *kernel,\n",
    "    int col,\n",
    "    int row,\n",
    "    int kernel_width\n",
    ");\n",
    "```\n",
    "\n",
    "Main training loop:\n",
    "```\n",
    "for (const auto& image : image_refs) {\n",
    "    input->setImage(image->data);\n",
    "    output->forward();\n",
    "    output->backward(learning_rate, nullptr);\n",
    "}\n",
    "```\n",
    "\n",
    "#### Results\n",
    "\n",
    "- **Training data**: 1,000 images (≈2% of CIFAR-10 training set).\n",
    "- **Test data**: Full 10,000-image CIFAR-10 test set.\n",
    "- **Reconstruction loss**:\n",
    "  - Training MSE loss ≈ **0.01**.\n",
    "  - Test MSE loss ≈ **0.01**.\n",
    "- This result is **surprising**, because the model was trained on a very small subset but still shows similar loss on the full test set.\n",
    "- Loss values are stable during evaluation.\n",
    "- Reconstructed images preserve overall structure but are blurry.\n",
    "- **Performance (CPU, 2% dataset only)**:\n",
    "  - Average epoch time: **59.35 seconds**\n",
    "  - Total training time: **11,869.94 seconds**\n",
    "- Total parameters: ~751,875, small enough for CPU memory.\n",
    "\n",
    "![CPU_comparison](resources/CPU_comparison.png)\n",
    "#### Key Takeaways\n",
    "- Even training on only 1,000 images, the autoencoder generalizes well in terms of MSE.\n",
    "- CPU performance is very slow, mainly due to convolution.\n",
    "- Conv2D is the main bottleneck.\n",
    "- These observations strongly motivate moving convolution and training to GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b2605",
   "metadata": {},
   "source": [
    "### Phase 2.5: SVM Integration\n",
    "\n",
    "#### Objectives\n",
    "- Use the trained encoder to extract image features.\n",
    "- Train an SVM classifier on these features.\n",
    "- Evaluate the full image classification pipeline.\n",
    "\n",
    "#### Implementation Details\n",
    "\n",
    "##### Feature Extraction\n",
    "- Only the **encoder** part of the autoencoder is used.\n",
    "- For each image, a forward pass is executed.\n",
    "- The encoder output is taken as the feature vector.\n",
    "- Feature size is **8 × 8 × 128 = 8192 dimensions**.\n",
    "- Features are extracted for:\n",
    "  - 50,000 training images\n",
    "  - 10,000 test images\n",
    "- Features and labels are saved into CSV files for later use.\n",
    "\n",
    "Feature extraction logic:\n",
    "```\n",
    "input->setImage(image.data);\n",
    "(*layers.rbegin())->forward();\n",
    "\n",
    "const float* enc_dev = encoder_layer->output();\n",
    "cudaMemcpy(enc_host.data(), enc_dev,\n",
    "           feature_size * sizeof(float),\n",
    "           cudaMemcpyDeviceToHost);\n",
    "```\n",
    "\n",
    "##### SVM Integration\n",
    "- Extracted features are loaded using cuDF.\n",
    "- Features are normalized using `StandardScaler`.\n",
    "- SVM is trained using cuML `SVC`, which runs on GPU.\n",
    "- This avoids implementing SVM from scratch and is fast.\n",
    "\n",
    "##### Hyperparameter Selection\n",
    "- Kernel: RBF\n",
    "- C = 10.0\n",
    "- gamma = \"scale\"\n",
    "- These values give good accuracy without long training time.\n",
    "\n",
    "SVM training code:\n",
    "```\n",
    "model = SVC(kernel='rbf', C=10.0, gamma='scale')\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "#### Results\n",
    "\n",
    "- **Feature extraction**:\n",
    "  - Training set: 50,000 images\n",
    "  - Test set: 10,000 images\n",
    "  - Feature size: 8192 per image\n",
    "- **SVM training time**: ~46 seconds\n",
    "- **Classification accuracy**:\n",
    "  - Training accuracy: **86.03%**\n",
    "  - Test accuracy: **67.56%**\n",
    "\n",
    "##### Confusion Matrix Summary\n",
    "- Vehicles (ship, car, truck, plane) are classified very well.\n",
    "- Animals (cat, dog, bird) have lower accuracy.\n",
    "- Strong confusion exists between similar animals, especially cat and dog.\n",
    "![Confusion Matrix](resources/ConfusionMatrix.png)\n",
    "#### Analysis\n",
    "- **Easiest classes**: ship, frog, plane, car.\n",
    "- **Hardest classes**: cat, bird, dog.\n",
    "- The confusion matrix shows most errors are between visually similar classes.\n",
    "- Test accuracy is slightly higher than the expected range (60–65%).\n",
    "\n",
    "#### Key Takeaways\n",
    "- The encoder learns meaningful and reusable features.\n",
    "- The two-stage approach (autoencoder + SVM) works well.\n",
    "- GPU-based feature extraction and SVM give good end-to-end performance.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
