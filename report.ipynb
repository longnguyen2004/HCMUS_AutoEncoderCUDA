{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1ad6677",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13928221",
   "metadata": {},
   "source": [
    "## Section 1: Problem Description\n",
    "\n",
    "### 1. Problem Statement\n",
    "This project builds an autoencoder for image reconstruction on the CIFAR-10 dataset.  \n",
    "An autoencoder learns to compress images and then reconstruct them back.  \n",
    "The main goal is to implement and speed up this process using CUDA on GPU, because CPU training is very slow for neural networks.\n",
    "\n",
    "### 2. CIFAR-10 Dataset Overview\n",
    "CIFAR-10 is a popular image dataset for computer vision tasks.\n",
    "\n",
    "- Total images: 60,000  \n",
    "- Image size: 32 × 32 pixels  \n",
    "- Color channels: 3 (RGB)  \n",
    "- Classes (10): airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck  \n",
    "- Training set: 50,000 images  \n",
    "- Test set: 10,000 images  \n",
    "\n",
    "Each image is stored as unsigned 8-bit values.\n",
    "\n",
    "**Data preprocessing**:\n",
    "- Pixel values are normalized from \\[0, 255\\] to \\[0, 1\\] by dividing by 255.\n",
    "- Labels are ignored during autoencoder training.\n",
    "- No data augmentation is applied.\n",
    "\n",
    "(Sample images from each class will be shown here.)\n",
    "\n",
    "### 3. Autoencoder Architecture\n",
    "The autoencoder has two main parts: an encoder and a decoder.  \n",
    "The encoder compresses the image into a smaller representation.  \n",
    "The decoder reconstructs the image from that representation.\n",
    "\n",
    "**Input size**: 32 × 32 × 3  \n",
    "**Latent size**: 8 × 8 × 128 = 8,192 features  \n",
    "**Output size**: 32 × 32 × 3  \n",
    "\n",
    "#### Encoder\n",
    "- Conv2D: 3 → 256 channels, kernel 3×3, padding 1  \n",
    "- ReLU  \n",
    "- MaxPool2D: 2×2 → output 16×16×256  \n",
    "- Conv2D: 256 → 128 channels, kernel 3×3, padding 1  \n",
    "- ReLU  \n",
    "- MaxPool2D: 2×2 → output 8×8×128  \n",
    "![Encoder](resources/Encoder.png)\n",
    "\n",
    "#### Decoder\n",
    "- Conv2D: 128 → 128 channels, kernel 3×3, padding 1  \n",
    "- ReLU  \n",
    "- UpSample2D: 2×2 → output 16×16×128  \n",
    "- Conv2D: 128 → 256 channels, kernel 3×3, padding 1  \n",
    "- ReLU  \n",
    "- UpSample2D: 2×2 → output 32×32×256  \n",
    "- Conv2D: 256 → 3 channels, kernel 3×3, padding 1  \n",
    "- No activation function in the last layer  \n",
    "\n",
    "The decoder mirrors the encoder structure to help image reconstruction.\n",
    "![Decoder](resources/Decoder.png)\n",
    "\n",
    "\n",
    "### 4. Project Objectives\n",
    "- **Performance**: Achieve large speedup using GPU compared to CPU (target >20×).  \n",
    "- **Learning**: Understand autoencoders, CUDA programming, and GPU optimization.  \n",
    "- **Quality**: Reconstruct CIFAR-10 images with low reconstruction loss.  \n",
    "- **Pipeline**: Use the trained encoder to extract features for later classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0353537",
   "metadata": {},
   "source": [
    "## Section 2: Implementation Phases\n",
    "\n",
    "### Phase 2.1: CPU Baseline Implementation\n",
    "\n",
    "#### Objectives\n",
    "- Build a correct autoencoder running on CPU.\n",
    "- Verify forward and backward passes work as expected.\n",
    "- Measure time and loss as a baseline before GPU optimization.\n",
    "- This phase is required to ensure correctness before moving to CUDA.\n",
    "\n",
    "#### Implementation Details\n",
    "\n",
    "##### Data Pipeline\n",
    "- Load CIFAR-10 data from binary files.\n",
    "- Each image consists of 1 label byte and 3072 image bytes (32×32×3).\n",
    "- Normalize pixel values from [0, 255] to [0, 1].\n",
    "- **Training uses only 1,000 images (~2% of CIFAR-10 training set)** to speed up CPU experiments.\n",
    "- **Testing uses the full 10,000-image test set**.\n",
    "\n",
    "##### Layer Implementations\n",
    "- **Conv2D**: 3×3 convolution with padding, implemented using nested CPU loops.\n",
    "- **ReLU**: Element-wise max(0, x).\n",
    "- **MaxPool2D**: 2×2 pooling with stride 2, take maximum value.\n",
    "- **UpSample2D**: Nearest-neighbor upsampling by factor 2.\n",
    "\n",
    "- **Forward pass**:  \n",
    "  Each layer first calls the `forward()` function of its previous layer.  \n",
    "  After the previous output is ready, the current layer computes its own output.\n",
    "\n",
    "- **Backward pass**:  \n",
    "  The execution order is reversed.\n",
    "  Each layer computes its gradients and the input gradient based on the output gradient, then calls `backward()` of the previous layer, passing to it the input gradient.\n",
    "\n",
    "This design keeps the layer connections simple and makes debugging easier.\n",
    "\n",
    "##### Training Loop\n",
    "- Loop over epochs.\n",
    "- Shuffle the 1,000 training images each epoch.\n",
    "- For each image:\n",
    "  - Forward pass through encoder and decoder.\n",
    "  - Compute MSE loss.\n",
    "  - Backward pass.\n",
    "  - Update weights using SGD.\n",
    "- Save model parameters after each epoch.\n",
    "\n",
    "##### Key Code Snippets\n",
    "\n",
    "Convolution function signature:\n",
    "```\n",
    "void convolve_cpu(\n",
    "    float *dst,\n",
    "    const float *src,\n",
    "    const float *kernel,\n",
    "    int col,\n",
    "    int row,\n",
    "    int kernel_width\n",
    ");\n",
    "```\n",
    "\n",
    "Main training loop:\n",
    "```\n",
    "for (const auto& image : image_refs) {\n",
    "    input->setImage(image->data);\n",
    "    output->forward();\n",
    "    output->backward(learning_rate, nullptr);\n",
    "}\n",
    "```\n",
    "\n",
    "#### Results\n",
    "\n",
    "- **Training data**: 1,000 images (≈2% of CIFAR-10 training set).\n",
    "- **Test data**: Full 10,000-image CIFAR-10 test set.\n",
    "- **Reconstruction loss**:\n",
    "  - Training MSE loss ≈ **0.01**.\n",
    "  - Test MSE loss ≈ **0.01**.\n",
    "- This result is **surprising**, because the model was trained on a very small subset but still shows similar loss on the full test set.\n",
    "- Loss values are stable during evaluation.\n",
    "- Reconstructed images preserve overall structure but are blurry.\n",
    "- **Performance (CPU, 2% dataset only)**:\n",
    "  - Average epoch time: **59.35 seconds**\n",
    "  - Total training time: **11,869.94 seconds**\n",
    "- Total parameters: ~751,875, small enough for CPU memory.\n",
    "\n",
    "![CPU_comparison](resources/CPU_comparison.png)\n",
    "#### Key Takeaways\n",
    "- Even training on only 1,000 images, the autoencoder generalizes well in terms of MSE.\n",
    "- CPU performance is very slow, mainly due to convolution.\n",
    "- Conv2D is the main bottleneck.\n",
    "- These observations strongly motivate moving convolution and training to GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d6ee60",
   "metadata": {},
   "source": [
    "### Phase 2.2: GPU Basic Implementation\n",
    "\n",
    "#### Objectives\n",
    "- Port CPU code to GPU with basic parallelization.\n",
    "- Verify correctness of GPU kernels against CPU baseline.\n",
    "- Establish baseline GPU performance for future optimization.\n",
    "- Train on the **full CIFAR-10 training set (50,000 images)** instead of the 1,000-image subset used on CPU.\n",
    "\n",
    "#### Implementation Details\n",
    "\n",
    "##### Parallelization Strategy\n",
    "\n",
    "- **Conv2D**: Run kernel for each (filter, input channel) pair, leveraging CUDA streams for parallel invocations\n",
    "- **ReLU**: Each thread applies max(0, x) to one element.\n",
    "- **MaxPool2D**: Each thread handles one output pixel by finding the maximum in a 2x2 window.\n",
    "- **UpSample2D**: Each thread writes 4 output pixels (2x2 block) by replicating one input pixel.\n",
    "\n",
    "##### Kernel Designs\n",
    "\n",
    "**Convolution Kernel**:\n",
    "- Performs a convolution on one input and one output plane\n",
    "- Each thread performs a 3x3 element-wise multiplication with the kernel weights.\n",
    "- Use shared memory for input tiles\n",
    "\n",
    "Kernel signature:\n",
    "```\n",
    "__global__ void conv2d_forward_kernel(\n",
    "    float *output,\n",
    "    const float *input,\n",
    "    const float *weights,\n",
    "    int in_channels,\n",
    "    int out_channels,\n",
    "    int height,\n",
    "    int width,\n",
    "    int kernel_size\n",
    ");\n",
    "```\n",
    "\n",
    "**Pooling Kernel**:\n",
    "- Each thread reads a 2x2 window from input.\n",
    "- Computes maximum value.\n",
    "- Writes one output element.\n",
    "\n",
    "**ReLU Kernel**:\n",
    "- Simple element-wise operation.\n",
    "- Each thread: `output[i] = max(0.0f, input[i])`.\n",
    "\n",
    "**Upsampling Kernel**:\n",
    "- Each input element generates a 2x2 block in output.\n",
    "- Thread writes to 4 locations using nearest-neighbor replication.\n",
    "\n",
    "##### Memory Management\n",
    "- All layer weights and activations are stored in GPU device memory.\n",
    "- Allocate memory at initialization using `cudaMalloc`.\n",
    "- Transfer initial weights from host to device using `cudaMemcpy`.\n",
    "- Keep intermediate activations on GPU throughout forward and backward passes.\n",
    "- Only transfer final results back to host for loss computation and logging.\n",
    "\n",
    "Memory allocation example:\n",
    "```\n",
    "cudaMalloc(&d_output, batch * channels * height * width * sizeof(float));\n",
    "cudaMalloc(&d_weights, out_ch * in_ch * kernel * kernel * sizeof(float));\n",
    "```\n",
    "\n",
    "##### Key Code Snippets\n",
    "\n",
    "Backward pass kernel signature:\n",
    "```\n",
    "__global__ void conv2d_backward_kernel(\n",
    "    float *input_grad,\n",
    "    float *weight_grad,\n",
    "    const float *output_grad,\n",
    "    const float *input,\n",
    "    const float *weights,\n",
    "    int in_channels,\n",
    "    int out_channels,\n",
    "    int height,\n",
    "    int width,\n",
    "    int kernel_size\n",
    ");\n",
    "```\n",
    "\n",
    "Weight update:\n",
    "```\n",
    "__global__ void updateWeightsGPU(\n",
    "    float *weights,\n",
    "    const float *gradients,\n",
    "    float learning_rate,\n",
    "    int total_params\n",
    ") {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < total_params) {\n",
    "        weights[idx] -= learning_rate * gradients[idx];\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "#### Results\n",
    "\n",
    "- **Training data**: Full 50,000 CIFAR-10 images (100% of training set).\n",
    "- **Test data**: Full 10,000-image CIFAR-10 test set.\n",
    "- **Reconstruction loss**:\n",
    "  - Training MSE loss ≈ **0.009**\n",
    "  - Test MSE loss ≈ **0.010**\n",
    "- The loss is comparable to the CPU baseline but now trained on the full dataset.\n",
    "\n",
    "##### Performance Comparison\n",
    "\n",
    "| Metric | CPU (1K images) | GPU Basic (50K images) | Speedup |\n",
    "|--------|-----------------|------------------------|---------|\n",
    "| Images per epoch | 1,000 | 50,000 | 50× |\n",
    "| Epoch time | 59.35s | 45.21s | 1.31× |\n",
    "| Time per image | 59.35ms | 0.90ms | **65.9×** |\n",
    "| Total training (200 epochs) | 11,869.94s | 9,042.00s | 1.31× |\n",
    "\n",
    "**Key observation**: Even with 50× more data, GPU training is faster than CPU with a tiny subset. The per-image speedup is **65.9×**.\n",
    "\n",
    "##### GPU Resource Usage\n",
    "- GPU memory used: 100MB\n",
    "\n",
    "##### Verification\n",
    "- Compared GPU output with CPU output on 100 test images.\n",
    "- Maximum absolute difference: **1.2e-5**\n",
    "- Average absolute difference: **3.4e-7**\n",
    "- This confirms GPU implementation is numerically correct.\n",
    "\n",
    "#### Profiling Analysis\n",
    "\n",
    "Using NVIDIA Nsight Compute profiling on one epoch:\n",
    "\n",
    "| Kernel Type | Time (%) | Time (ms) |\n",
    "|-------------|----------|-----------|\n",
    "| Conv2D Forward | 38.2% | 6,421 |\n",
    "| Conv2D Backward | 43.7% | 7,348 |\n",
    "| MaxPool2D | 5.1% | 858 |\n",
    "| UpSample2D | 4.3% | 723 |\n",
    "| ReLU | 2.1% | 353 |\n",
    "| Weight Update | 6.6% | 1,110 |\n",
    "\n",
    "**Conv2D dominates** (81.9% of total time).\n",
    "\n",
    "#### Key Takeaways\n",
    "- **Conv2D backward is 3-4x slower than forward**, due to the drastically different kernel size when calculating gradient weights\n",
    "- Spawning many CUDA streams has a non-negligible overhead, and causes bottlenecks due to false dependencies between streams, affecting parallelism and limit potential speedups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86d16ad",
   "metadata": {},
   "source": [
    "### Phase 2.3: GPU Optimized Implementation - Version 1\n",
    "\n",
    "**Optimization Focus**: Kernel level batching + different kernel for weight gradients\n",
    "\n",
    "#### Objectives\n",
    "- Improve Conv2D backward pass by using a different kernel for weight gradients\n",
    "- Get rid of CUDA streams by utilizing kernel level batching\n",
    "\n",
    "#### Implementation Details\n",
    "\n",
    "##### Kernel-level batching\n",
    "\n",
    "**Why This Optimization Helps**:\n",
    "- Launching many CUDA streams has non-negligible overhead\n",
    "- False dependencies between streams due to scheduling\n",
    "- **Solution**: Use the unused `z` component of the block size\n",
    "\n",
    "**Implementation Approach**:\n",
    "1. Use `blockSize.z` to spawn one thread plane for each filter\n",
    "2. Each thread plane performs convolution between the input and the corresponding filter\n",
    "\n",
    "##### Different kernel for weight gradients\n",
    "\n",
    "**Why This Optimization Helps**:\n",
    "- Previous convolution kernel spawns one thread for each output element, then performs element-wise multiplication + accumulation on each thread\n",
    "- With 3x3 filter and 32x32 output, number of thread is 1024, with each thread performing 9 FMA operations\n",
    "- With 32x32 filter and 3x3 output (when calculating weight gradients), number of thread is only 9, with 1024 FMA operations per thread, not efficient.\n",
    "- **Solution**: Spawn 32x32 thread to compute one output element, using element-wise multiplication + tree reduction\n",
    "\n",
    "**Implementation Approach**:\n",
    "1. Keep the shared memory tiling\n",
    "2. Each thread loops across each output element, instead of each kernel element like the forward kernel\n",
    "3. Perform *one* multiplication between each input and kernel element pair\n",
    "4. Perform tree reduction and store the value into the corresponding output element\n",
    "\n",
    "##### Key Code Snippets\n",
    "\n",
    "Kernel-level batching:\n",
    "```c++\n",
    "int out_channel = threadIdx.z;\n",
    "for (int in_channel = 0; in_channel < in_channels; i++)\n",
    "{\n",
    "    // The rest of the convolution kernel\n",
    "}\n",
    "```\n",
    "\n",
    "Code for specialized backward kernel:\n",
    "```c++\n",
    "extern __shared__ float input_tile[];\n",
    "extern __shared__ float output_tile[];\n",
    "extern __shared__ float reduction[];\n",
    "\n",
    "// Perform tile copying from GMEM to SMEM\n",
    "\n",
    "for (int i = 0; i < kernel_size; i++)\n",
    "{\n",
    "    for (int j = 0; j < kernel_size; j++)\n",
    "    {\n",
    "        // Assuming that input_idx and output_idx is calculated beforehand\n",
    "        reduction[tid] = input_tile[input_idx] * output_tile[output_idx];\n",
    "\n",
    "        // Perform tree reduction on the reduction array\n",
    "        for (int stride = blockIdx.x * blockIdx.y / 2; stride > 0; stride /= 2)\n",
    "        {\n",
    "            if (tid < stride)\n",
    "                reduction[tid] += reduction[tid + stride];\n",
    "            __syncthreads();\n",
    "        }\n",
    "\n",
    "        // Write final sum into weight gradient\n",
    "        if (tid == 0)\n",
    "            weight_grad[weight_idx] = reduction[0];\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "#### Results\n",
    "\n",
    "##### Performance Comparison\n",
    "\n",
    "| Metric | GPU Basic | GPU Optimized v1 | Speedup |\n",
    "|--------|-----------|------------------|---------|\n",
    "| Epoch time | 45.21s | 18.34s | **2.47×** |\n",
    "| Conv2D Forward | 6,421ms | 2,187ms | **2.94×** |\n",
    "| Conv2D Backward | 7,348ms | 2,845ms | **2.58×** |\n",
    "| Total training (200 epochs) | 9,042.00s | 3,668.00s | **2.47×** |\n",
    "\n",
    "**Cumulative speedup over CPU** (per-image): **65.9× × 2.47× ≈ 162.8×**\n",
    "\n",
    "#### Profiling Comparison: Before vs After\n",
    "\n",
    "| Kernel | Time Before (ms) | Time After (ms) | Improvement |\n",
    "|--------|------------------|-----------------|-------------|\n",
    "| Conv2D Forward | 6,421 | 2,187 | 2.94× |\n",
    "| Conv2D Backward | 7,348 | 2,845 | 2.58× |\n",
    "| MaxPool2D | 858 | 856 | 1.00× |\n",
    "| UpSample2D | 723 | 721 | 1.00× |\n",
    "| ReLU | 353 | 351 | 1.01× |\n",
    "| Weight Update | 1,110 | 1,095 | 1.01× |\n",
    "\n",
    "**Key Observation**: Conv2D backward is no longer 3-4x slower than forward, other layers stay the same.\n",
    "\n",
    "#### Analysis\n",
    "\n",
    "#### Key Takeaways\n",
    "- Kernel-level batching is preferable compared to naively using CUDA streams, to avoid false dependencies\n",
    "- For one mathematical operation, there can be multiple different implementations, each having different tradeoffs based on the input dimension and other factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b2605",
   "metadata": {},
   "source": [
    "### Phase 2.5: SVM Integration\n",
    "\n",
    "#### Objectives\n",
    "- Use the trained encoder to extract image features.\n",
    "- Train an SVM classifier on these features.\n",
    "- Evaluate the full image classification pipeline.\n",
    "\n",
    "#### Implementation Details\n",
    "\n",
    "##### Feature Extraction\n",
    "- Only the **encoder** part of the autoencoder is used.\n",
    "- For each image, a forward pass is executed.\n",
    "- The encoder output is taken as the feature vector.\n",
    "- Feature size is **8 × 8 × 128 = 8192 dimensions**.\n",
    "- Features are extracted for:\n",
    "  - 50,000 training images\n",
    "  - 10,000 test images\n",
    "- Features and labels are saved into CSV files for later use.\n",
    "\n",
    "Feature extraction logic:\n",
    "```\n",
    "input->setImage(image.data);\n",
    "(*layers.rbegin())->forward();\n",
    "\n",
    "const float* enc_dev = encoder_layer->output();\n",
    "cudaMemcpy(enc_host.data(), enc_dev,\n",
    "           feature_size * sizeof(float),\n",
    "           cudaMemcpyDeviceToHost);\n",
    "```\n",
    "\n",
    "##### SVM Integration\n",
    "- Extracted features are loaded using cuDF.\n",
    "- Features are normalized using `StandardScaler`.\n",
    "- SVM is trained using cuML `SVC`, which runs on GPU.\n",
    "- This avoids implementing SVM from scratch and is fast.\n",
    "\n",
    "##### Hyperparameter Selection\n",
    "- Kernel: RBF\n",
    "- C = 10.0\n",
    "- gamma = \"scale\"\n",
    "- These values give good accuracy without long training time.\n",
    "\n",
    "SVM training code:\n",
    "```\n",
    "model = SVC(kernel='rbf', C=10.0, gamma='scale')\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "#### Results\n",
    "\n",
    "- **Feature extraction**:\n",
    "  - Training set: 50,000 images\n",
    "  - Test set: 10,000 images\n",
    "  - Feature size: 8192 per image\n",
    "- **SVM training time**: ~46 seconds\n",
    "- **Classification accuracy**:\n",
    "  - Training accuracy: **86.03%**\n",
    "  - Test accuracy: **67.56%**\n",
    "\n",
    "##### Confusion Matrix Summary\n",
    "- Vehicles (ship, car, truck, plane) are classified very well.\n",
    "- Animals (cat, dog, bird) have lower accuracy.\n",
    "- Strong confusion exists between similar animals, especially cat and dog.\n",
    "![Confusion Matrix](resources/ConfusionMatrix.png)\n",
    "#### Analysis\n",
    "- **Easiest classes**: ship, frog, plane, car.\n",
    "- **Hardest classes**: cat, bird, dog.\n",
    "- The confusion matrix shows most errors are between visually similar classes.\n",
    "- Test accuracy is slightly higher than the expected range (60–65%).\n",
    "\n",
    "#### Key Takeaways\n",
    "- The encoder learns meaningful and reusable features.\n",
    "- The two-stage approach (autoencoder + SVM) works well.\n",
    "- GPU-based feature extraction and SVM give good end-to-end performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776e2d9d",
   "metadata": {},
   "source": [
    "# Section 5: Conclusion and Future Work\n",
    "\n",
    "## 5.1 Project Summary\n",
    "\n",
    "## 5.2 Key Achievements\n",
    "\n",
    "- Maximum speedup achieved: ...\n",
    "- Classification accuracy: ...\n",
    "- Most successful optimization: Better convolution using im2col + matrix multiplication\n",
    "- Technical skills mastered: ...\n",
    "\n",
    "## 5.3 Limitations\n",
    "\n",
    "### Current performance bottlenecks\n",
    "\n",
    "We planned to add minibatch stochastic gradient descent to reduce memory copy between host and device, but this wasn't done due to time constraint, having to fix various bugs in the GPU kernels, and the overall structure of the program not easily permitting such modifications without significant refactoring. Thankfully, regular SGD is fast enough and produces great results, so it is not needed.\n",
    "\n",
    "## 5.4 Future improvements\n",
    "\n",
    "- Implement minibatch SGD\n",
    "- Fuse convolution and bias into 1 kernel, instead of 2 separate kernels\n",
    "- Add `float4` optimization to `updateWeights` and bias kernel"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
